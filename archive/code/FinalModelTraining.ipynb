{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.metrics import  accuracy_score, mean_squared_error\n",
    "from sklearn.base import clone\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "class CustomTimeSeriesCV(BaseCrossValidator):\n",
    "    \"\"\"Creates an iterator that contains the indices from each dataset based on the years given\"\"\"\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for train_years, test_years in self.years:\n",
    "            train_indices = np.where(X['year'].isin(train_years))[0]\n",
    "            test_indices = np.where(X['year'].isin(test_years))[0]\n",
    "            yield train_indices, test_indices\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return len(self.years) \n",
    "    \n",
    "def bootstrap(group, n=None):\n",
    "    if n is None:\n",
    "        n = len(group)\n",
    "    return group.sample(n, replace=True)\n",
    "\n",
    "#Categorical features that need to be one-hot encoded    \n",
    "one_hot_fts = ['office_type']\n",
    "\n",
    "#Rating is the only ordinal feature\n",
    "ordinal_fts = ['final_rating']\n",
    "ordinal_fts_ranking = ['Safe R', 'Likely R', 'Leans R', 'Toss-up', 'Leans D', 'Likely D', 'Safe D']\n",
    "\n",
    "#Cont features that should be pass-throughed (aznd later scaled)\n",
    "cont_fts = [\n",
    "    \"open_seat\", \"incumbent_differential\", \"special\", \"absenteeexcusereq\", \"pollhours\", \"avgpollhours\", \"minpollhours\",\n",
    "    \"regdeadlines\", \"voteridlaws\", \"novoterid\", \"noallmailvote\", \"noearlyvote\", \"nofelonreg\",\n",
    "    \"nofelonsregafterincar\", \"nonstrictid\", \"nonstrictphoto\", \"nopollplacereg\", \"nopr\", \"nosamedayreg\",\n",
    "    \"nostateholiday\", \"pr16\", \"pr17\", \"pr175\", \"pr60\", \"pr90\", \"strictid\", \"strictphoto\", \"covi_num\",\n",
    "    \"prev_dem_gen_tp\", \"prev_gen_margin\", \"weighted_genpoll\", \"weighted_genpoll_lower\",\n",
    "    \"weighted_genpoll_upper\", \"unweighted_genpoll\", \"mean_specials_differential\", \n",
    "    \"house_chamber_margin\", \"senate_chamber_margin\", \"previous_cci\", \"current_cci\", \"change_cci\",\n",
    "    \"previous_gas\", \"current_gas\", \"change_gas\", \"previous_unemployment\", \"current_unemployment\",\n",
    "    \"change_unemployment\",  \"receipts\", \"from_committee_transfers\", \"disbursements\",\n",
    "    \"to_committee_transfers\", \"beginning_cash\", \"ending_cash\", \"candidate_contributions\",\n",
    "    \"individual_contributions\", \"unconvinced_pct\", \"phone_unweighted\", \"online_unweighted\", \"num_polls\",\n",
    "    \"unweighted_estimate\", \"unweighted_ci_lower\", \"unweighted_ci_upper\", \"weighted_estimate\",\n",
    "    \"weighted_ci_lower\", \"weighted_ci_upper\", \"white_pct\", \"black_pct\", \"asian_pct\", \"hispanic_pct\",\n",
    "    \"median_income\", \"impoverished_pct\", \"median_age\", \"renting_pct\", \"inflation\", \"isMidterm\",\n",
    "    \"genballot_predicted_margin\", \"genballot_predicted_lower\", \"genballot_predicted_upper\",\n",
    "    \"poll_fundamental_agree\",  'receipts_DEM', 'receipts_REP', 'disbursements_DEM', 'disbursements_REP'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "def optima_model(mean_model, std_model, mean_param_dict, std_param_dict, X, y, mean_kwargs, std_kwargs):\n",
    "    \"\"\"Performs hyperparameter optimization for a a given bootstrapped X \n",
    "    ## Parameters:\n",
    "    mean_model: sklearnable model. We use LGBMRegressor. This model trains for the point estimates.\n",
    "    std_model: sklearnable model. We use LGBMRegressor. This model trains for the standard deviations after the point estimates are trained.\n",
    "    param_dict: dictionary of hyperparameters to optimize\n",
    "    X: DataFrame with features\n",
    "    y: Series with target variable\"\"\"\n",
    "        \n",
    "    X_other, y_other = X.loc[X['year'] <= 2022, :], y.loc[X['year'] <= 2022]\n",
    "    X_train, X_test, y_train, y_test = (X.loc[X['year'] < 2022, :], X.loc[X['year'] == 2022, :], \n",
    "                                        y.loc[X['year'] < 2022], y.loc[X['year'] == 2022])\n",
    "    \n",
    "    # Create fold structure so we can make a custom cross-validation for time-series\n",
    "    folds = [\n",
    "        (range(2002, 2010, 2), [2010, 2012]),\n",
    "        (range(2002, 2014, 2), [2014, 2016]),\n",
    "        (range(2002, 2018, 2), [2018, 2020])\n",
    "    ]\n",
    "\n",
    "    cv = CustomTimeSeriesCV(folds)\n",
    "        \n",
    "    #Preprocessing data: no need to scale data, because we use tree-based models which are monotonic-scale-invariant\n",
    "    #Because we don't need to scale data, we don't have to include the column transformer in the final saved model\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts),\n",
    "        ('ord', OrdinalEncoder(categories = [ordinal_fts_ranking], handle_unknown='use_encoded_value', \n",
    "                               unknown_value=np.nan), ordinal_fts),\n",
    "        ('num', 'passthrough', cont_fts)])\n",
    "    \n",
    "    #--- First, we optimize the mean model ---\n",
    "    def mean_objective(params):\n",
    "        \"Function that takes in hyperparameters and returns loss, that Hyperopt will minimize.\"        \n",
    "        testing_loss = []\n",
    "        accuracies = []\n",
    "        for train_idx, test_idx in cv.split(X_train):\n",
    "            reg = mean_model(**params, **mean_kwargs)\n",
    "            pipe = Pipeline(steps = [\n",
    "                ('preprocessing', preprocessor), \n",
    "                ('model', reg)])\n",
    "            \n",
    "            \"\"\"Goes through each fold and calculates loss.\"\"\"\n",
    "            pipe.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "            \n",
    "            predictions = pipe.predict(X_train.iloc[test_idx])\n",
    "            testing_loss.append(mean_squared_error(y_train.iloc[test_idx], predictions, squared = False))\n",
    "            accuracies.append(accuracy_score(np.sign(y_train.iloc[test_idx]), np.sign(predictions)))\n",
    "        print(accuracies)\n",
    "         \n",
    "        return {'loss': np.mean(testing_loss), 'status': STATUS_OK}\n",
    "\n",
    "    \"Hyperopt uses the TPE algorithm to optimize hyperparameters. We use the no_progress_loss function to stop early if we don't see progress.\"\n",
    "    mean_best_params = fmin(fn=mean_objective,\n",
    "                    space=mean_param_dict,\n",
    "                    algo=tpe.suggest,\n",
    "                    trials=Trials(),\n",
    "                    early_stop_fn = no_progress_loss(1))\n",
    "    \n",
    "    mean_best_model = mean_model(**mean_best_params, **mean_kwargs)\n",
    "    mean_best_pipe = Pipeline(steps = [\n",
    "        ('preprocessing', preprocessor), \n",
    "        ('model', mean_best_model)])\n",
    "    \n",
    "           \n",
    "    #--- Now, we optimize the standard deviation model ---\n",
    "    #Begin by predicting the training data via the mean model\n",
    "    def neg_log_likelihood(y, y_pred, y_std):\n",
    "        return np.mean(0.5 * np.log(2 * np.pi * y_std ** 2) + ((y - y_pred) ** 2 / (2 * y_std ** 2)))\n",
    "\n",
    "    \n",
    "    mean_best_pipe.fit(X_train, y_train)\n",
    "    means_predicted = mean_best_pipe.predict(X_train)\n",
    "    std_y_train = np.abs(y_train - means_predicted)\n",
    "    \n",
    "    def std_objective(params):\n",
    "        \"Function that takes in hyperparameters and returns loss, that Hyperopt will minimize.\"        \n",
    "        testing_loss = []\n",
    "        z_scores = []\n",
    "        for train_idx, test_idx in cv.split(X_train):                \n",
    "            \n",
    "            std_reg = std_model(**params, **std_kwargs)\n",
    "            pipe = Pipeline(steps = [\n",
    "                ('preprocessing', preprocessor), \n",
    "                ('model', std_reg)])\n",
    "            \n",
    "            \"\"\"Goes through each fold and calculates loss.\"\"\"\n",
    "            pipe.fit(X_train.iloc[train_idx], std_y_train.iloc[train_idx])\n",
    "            \n",
    "            std_predictions = pipe.predict(X_train.iloc[test_idx])\n",
    "            testing_loss.append(neg_log_likelihood(y_train.iloc[test_idx], means_predicted[test_idx], std_predictions))\n",
    "            z_scores.append(np.mean(abs(y_train.iloc[test_idx] - means_predicted[test_idx]) / std_predictions))\n",
    "        print(z_scores)\n",
    "                     \n",
    "        return {'loss': np.mean(testing_loss), 'status': STATUS_OK}\n",
    "    \n",
    "    \"Hyperopt uses the TPE algorithm to optimize hyperparameters. We use the no_progress_loss function to stop early if we don't see progress.\"\n",
    "    std_best_params = fmin(fn=std_objective,\n",
    "                    space=std_param_dict,\n",
    "                    algo=tpe.suggest,\n",
    "                    trials=Trials(),\n",
    "                    early_stop_fn = no_progress_loss(1))\n",
    "    \n",
    "    #once we get the best params for each, we train each sequentially and then return the fitted versions.\n",
    "    mean_best_pipe.fit(X_other, y_other)\n",
    "    mean_predictions = mean_best_pipe.predict(X_other)\n",
    "    \n",
    "    std_y_other = np.abs(y_other - mean_predictions)\n",
    "    std_best_model = std_model(**std_best_params, **std_kwargs)\n",
    "    std_best_pipe = Pipeline(steps = [\n",
    "        ('preprocessing', preprocessor), \n",
    "        ('model', std_best_model)])\n",
    "    std_best_pipe.fit(X_other, std_y_other)\n",
    "    \n",
    "    #Returns 2 fitted models\n",
    "    return mean_best_pipe, std_best_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9197465681098205, 0.9424460431654677, 0.9126730564430245]           \n",
      "[0.9503695881731784, 0.9532374100719424, 0.9531416400425985]                                                      \n",
      "[0.9165786694825766, 0.9436450839328537, 0.9148029818956337]                                                      \n",
      "  0%|          | 3/9223372036854775807 [00:29<25404194780371717:41:20,  9.92s/trial, best loss: 8.572827609743522]\n",
      "[1.5550199287260107, 1.6407764381597234, 1.373680168895199]            \n",
      "[0.9836432329693462, 1.0664403983683504, 0.8865585514169935]                                                      \n",
      "[1.0912418658899485, 1.223466440065979, 0.9636387921025459]                                                       \n",
      "  0%|          | 3/9223372036854775807 [00:10<8608863906351750:15:28,  3.36s/trial, best loss: 3.132743630226644]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../cleaned_data/Engineered Dataset.csv\")\n",
    "data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "X = data.drop(columns = ['margin'])\n",
    "y = data['margin']\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts),\n",
    "        ('ord', OrdinalEncoder(categories = [ordinal_fts_ranking], handle_unknown='use_encoded_value', \n",
    "                               unknown_value=np.nan), ordinal_fts),\n",
    "        ('num', 'passthrough', cont_fts)])\n",
    "\n",
    "names_for_monotonicity = preprocessor.fit(X).get_feature_names_out()\n",
    "before_processing_monotonic_columns = ['incumbent_differential', \"receipts\", \"from_committee_transfers\", \"disbursements\",'genballot_predicted_margin', \n",
    "                                       'specials_predicted_margin', 'unweighted_estimate', 'weighted_estimate', 'receipts_genballot_interaction',\n",
    "                                       'disbursements_genballot_interaction', 'poll_fundamental_average', 'genballot_predicted_lower', \n",
    "                                       'genballot_predicted_upper']\n",
    "\n",
    "monotonic_columns = ['num__' + name for name in before_processing_monotonic_columns] + ['ord__final_rating']\n",
    "\n",
    "monotone_constraints = [1 if name in monotonic_columns else 0 for name in names_for_monotonicity]\n",
    "\n",
    "# Define the search space for Hyperopt\n",
    "mean_param_lgbm = {\n",
    "    'num_leaves': hp.randint('num_leaves', 20, 70),  # Reduced the upper limit, \n",
    "    'n_estimators': hp.randint('n_estimators', 50, 200),  # Increased the range\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -2),  # Equivalent to about 0.0001 to 0.01\n",
    "    'subsample_for_bin': hp.randint('subsample_for_bin', 20000, 200000),  # Narrowed the range\n",
    "    'min_data_in_bin': hp.randint('min_data_in_bin', 1, 10), \n",
    "    'min_data_in_leaf': hp.randint('min_data_in_leaf', 1, 10),  # Reduced the upper limit\n",
    "    'min_child_samples': hp.randint('min_child_samples', 20, 150),  # Increased the range for more regularization\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.5),  # Increased upper limit for L1 regularization\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.5),  # Increased upper limit for L2 regularization\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.8),  # Reduced the upper limit\n",
    "    'subsample': hp.uniform('subsample', 0.5, 0.8),  # Reduced the upper limit for more randomness\n",
    "    'max_depth': hp.randint('max_depth', 2, 10),  # Added max_depth for additional control\n",
    "}\n",
    "\n",
    "mean_kwargs = {\n",
    "    'boosting_type': 'dart', \n",
    "    'monotone_constraints': monotone_constraints,\n",
    "    'monotone_constraints_method': 'advanced', \n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "std_param_lgbm = {\n",
    "    'num_leaves': hp.randint('num_leaves', 20, 70),  # Reduced the upper limit, \n",
    "    'n_estimators': hp.randint('n_estimators', 50, 200),  # Increased the range\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -2),  # Equivalent to about 0.0001 to 0.01\n",
    "    'subsample_for_bin': hp.randint('subsample_for_bin', 20000, 200000),  # Narrowed the range\n",
    "    'min_data_in_bin': hp.randint('min_data_in_bin', 1, 10), \n",
    "    'min_data_in_leaf': hp.randint('min_data_in_leaf', 1, 10),  # Reduced the upper limit\n",
    "    'min_child_samples': hp.randint('min_child_samples', 20, 150),  # Increased the range for more regularization\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.5),  # Increased upper limit for L1 regularization\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.5),  # Increased upper limit for L2 regularization\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.8),  # Reduced the upper limit\n",
    "    'subsample': hp.uniform('subsample', 0.5, 0.8),  # Reduced the upper limit for more randomness\n",
    "    'max_depth': hp.randint('max_depth', 2, 10),  # Added max_depth for additional control\n",
    "}\n",
    "\n",
    "std_kwargs = {\n",
    "    'boosting_type': 'dart', \n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "#Optimize the model\n",
    "mean_model, std_model = optima_model(lgb.LGBMRegressor, lgb.LGBMRegressor, mean_param_lgbm, std_param_lgbm, X, y, mean_kwargs, std_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong results_2020,       Unnamed0  year state  district office_type open_seat  \\\n",
      "3726      3727  2022    CA        13       House      True   \n",
      "3767      3768  2022    CO         8       House      True   \n",
      "3880      3881  2022    MI         7       House      True   \n",
      "3921      3922  2022    NC        13       House      True   \n",
      "3941      3942  2022    NM         2       House     False   \n",
      "3945      3946  2022    NV         3       House     False   \n",
      "3949      3950  2022    NY         3       House      True   \n",
      "3950      3951  2022    NY         4       House      True   \n",
      "3961      3962  2022    NY        17       House      True   \n",
      "3971      3972  2022    OH         1       House     False   \n",
      "4002      4003  2022    PA         7       House     False   \n",
      "4003      4004  2022    PA         8       House      True   \n",
      "4010      4011  2022    PA        17       House      True   \n",
      "4064      4065  2022    VA         2       House     False   \n",
      "4077      4078  2022    WA         3       House      True   \n",
      "4839      4840  2022    PA         0      Senate      True   \n",
      "5179      5180  2022    AZ         0    Governor      True   \n",
      "5190      5191  2022    KS         0    Governor     False   \n",
      "5211      5212  2022    WI         0    Governor     False   \n",
      "\n",
      "      incumbent_differential  special  absenteeexcusereq  pollhours  ...  \\\n",
      "3726                0.000000    False                0.0        0.0  ...   \n",
      "3767                0.000000    False                0.0        1.0  ...   \n",
      "3880                0.000000    False                0.0        7.0  ...   \n",
      "3921                0.000000    False                0.0        7.0  ...   \n",
      "3941                3.332214    False                0.0        8.0  ...   \n",
      "3945                4.084506    False                0.0        1.0  ...   \n",
      "3949                0.000000    False                1.0        5.0  ...   \n",
      "3950                0.000000    False                1.0        5.0  ...   \n",
      "3961                0.000000    False                1.0        5.0  ...   \n",
      "3971                0.134220    False                0.0        7.0  ...   \n",
      "4002                1.634390    False                0.0        7.0  ...   \n",
      "4003                0.000000    False                0.0        7.0  ...   \n",
      "4010                0.000000    False                0.0        7.0  ...   \n",
      "4064                6.962527    False                0.0        7.0  ...   \n",
      "4077                0.000000    False                0.0        0.0  ...   \n",
      "4839                0.000000    False                0.0        7.0  ...   \n",
      "5179                0.000000    False                0.0        7.0  ...   \n",
      "5190               28.018129    False                0.0        8.0  ...   \n",
      "5211                0.455868    False                0.0        7.0  ...   \n",
      "\n",
      "      receipts_genballot_interaction  disbursements_genballot_interaction  \\\n",
      "3726                        9.895731                             9.866853   \n",
      "3767                       -0.780174                            -0.780030   \n",
      "3880                       -4.802026                            -6.035575   \n",
      "3921                        8.827763                             8.968859   \n",
      "3941                       -0.618638                            -0.599001   \n",
      "3945                        3.518919                             3.578973   \n",
      "3949                        1.769245                             2.056848   \n",
      "3950                        5.965089                             6.415364   \n",
      "3961                        5.855221                             6.352882   \n",
      "3971                        0.308841                             0.253704   \n",
      "4002                       -0.270957                            -0.235477   \n",
      "4003                       -2.154911                            -2.217542   \n",
      "4010                       -1.270769                            -1.378727   \n",
      "4064                        0.939438                             0.927563   \n",
      "4077                        8.733655                             8.941113   \n",
      "4839                        0.038698                            -0.045053   \n",
      "5179                             NaN                                  NaN   \n",
      "5190                             NaN                                  NaN   \n",
      "5211                             NaN                                  NaN   \n",
      "\n",
      "      democrat_in_presidency  gas_democrat_interaction  \\\n",
      "3726                    True                     3.769   \n",
      "3767                    True                     3.769   \n",
      "3880                    True                     3.769   \n",
      "3921                    True                     3.769   \n",
      "3941                    True                     3.769   \n",
      "3945                    True                     3.769   \n",
      "3949                    True                     3.769   \n",
      "3950                    True                     3.769   \n",
      "3961                    True                     3.769   \n",
      "3971                    True                     3.769   \n",
      "4002                    True                     3.769   \n",
      "4003                    True                     3.769   \n",
      "4010                    True                     3.769   \n",
      "4064                    True                     3.769   \n",
      "4077                    True                     3.769   \n",
      "4839                    True                     3.769   \n",
      "5179                    True                     3.769   \n",
      "5190                    True                     3.769   \n",
      "5211                    True                     3.769   \n",
      "\n",
      "      cci_democrat_interaction  poll_fundamental_agree  mean_prediction  \\\n",
      "3726                      58.6                     NaN        13.544179   \n",
      "3767                      58.6                     1.0        -5.567641   \n",
      "3880                      58.6                    -1.0        -0.387303   \n",
      "3921                      58.6                     0.0        -3.860561   \n",
      "3941                      58.6                    -1.0        -3.434198   \n",
      "3945                      58.6                    -1.0        -1.740799   \n",
      "3949                      58.6                     NaN         4.961696   \n",
      "3950                      58.6                     NaN        11.312534   \n",
      "3961                      58.6                     1.0         7.290226   \n",
      "3971                      58.6                    -1.0        -1.417641   \n",
      "4002                      58.6                     1.0        -1.665677   \n",
      "4003                      58.6                    -1.0        -2.417077   \n",
      "4010                      58.6                     1.0        -2.124435   \n",
      "4064                      58.6                    -1.0         0.919907   \n",
      "4077                      58.6                     1.0        -8.433608   \n",
      "4839                      58.6                    -1.0        -2.617098   \n",
      "5179                      58.6                     1.0        -3.701360   \n",
      "5190                      58.6                     1.0        -1.201745   \n",
      "5211                      58.6                     1.0        -3.521867   \n",
      "\n",
      "      std_prediction   z-value  probability of being wrong  \n",
      "3726        4.462412  3.035170                    0.001202  \n",
      "3767        3.397295  1.638845                    0.050623  \n",
      "3880        2.927791  0.132285                    0.447379  \n",
      "3921        2.842081  1.358357                    0.087175  \n",
      "3941        3.195247  1.074783                    0.141236  \n",
      "3945        3.417615  0.509361                    0.305250  \n",
      "3949        4.259149  1.164950                    0.122020  \n",
      "3950        4.259149  2.656055                    0.003953  \n",
      "3961        3.487967  2.090107                    0.018304  \n",
      "3971        3.042940  0.465879                    0.320651  \n",
      "4002        3.292861  0.505845                    0.306483  \n",
      "4003        3.021665  0.799916                    0.211880  \n",
      "4010        3.397295  0.625331                    0.265877  \n",
      "4064        3.319920  0.277087                    0.390857  \n",
      "4077        3.107059  2.714338                    0.003320  \n",
      "4839        2.618687  0.999393                    0.158802  \n",
      "5179        2.847106  1.300043                    0.096793  \n",
      "5190        2.934401  0.409537                    0.341073  \n",
      "5211        2.550155  1.381041                    0.083633  \n",
      "\n",
      "[19 rows x 112 columns]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "mean_model.fit(X.loc[X['year'] < 2022], y.loc[X['year'] < 2022])\n",
    "mean_preds_for_training = mean_model.predict(X.loc[X['year'] < 2022])\n",
    "std_model.fit(X.loc[X['year'] < 2022], np.abs(y.loc[X['year'] < 2022] - mean_preds_for_training))\n",
    "\n",
    "mean_preds = mean_model.predict(X.loc[X['year'] == 2022])\n",
    "std_preds = std_model.predict(X.loc[X['year'] == 2022])\n",
    "wrong_indices = np.not_equal(np.sign(y.loc[X['year'] == 2022]), np.sign(mean_preds))\n",
    "wrong_results_2022 = X.loc[X['year'] == 2022].loc[wrong_indices]\n",
    "wrong_results_2022['mean_prediction'] = mean_preds[wrong_indices]\n",
    "wrong_results_2022['std_prediction'] = std_preds[wrong_indices]\n",
    "wrong_results_2022['z-value'] = np.abs(wrong_results_2022['mean_prediction']) / std_preds[wrong_indices]\n",
    "wrong_results_2022['probability of being wrong'] = 1 - norm.cdf(wrong_results_2022['z-value'])\n",
    "\n",
    "print(f\"Wrong results_2020, {wrong_results_2022}\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
