{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, BaseCrossValidator\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, max_error, accuracy_score, median_absolute_error, make_scorer\n",
    "import xgboost\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from hyperopt.pyll.base import Apply\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "from missforest.missforest import MissForest\n",
    "\n",
    "#Creating a custom time series cross-validator\n",
    "class CustomTimeSeriesCV(BaseCrossValidator):\n",
    "    \"\"\"Creates an iterator that contains the indices from each dataset based on the years given\"\"\"\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for train_years, test_years in self.years:\n",
    "            train_indices = np.where(X['year'].isin(train_years))[0]\n",
    "            test_indices = np.where(X['year'].isin(test_years))[0]\n",
    "            yield train_indices, test_indices\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return len(self.years)\n",
    "\n",
    "class MissForestImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_iter = 20):\n",
    "        self.model = MissForest(max_iter = max_iter)\n",
    "    \n",
    "    def get_params(self, deep: bool = False) -> dict:\n",
    "        return {'max_iter': self.model.max_iter}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X)\n",
    "        return self  # Return self to enable chaining\n",
    "\n",
    "    def transform(self, X):\n",
    "        # MissForest's transform method is actually predict in most cases\n",
    "        X_imputed = self.model.transform(X)\n",
    "        return X_imputed\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.model.fit_transform(X)\n",
    " \n",
    "    \n",
    "def missing_kernel(X, Y):\n",
    "    # Calculate pairwise squared differences using broadcasting, with an extra dimension for vectorization\n",
    "    diffs = X[:, np.newaxis, :] - Y[np.newaxis, :, :]\n",
    "    \n",
    "    # Create masks for non-missing values (True for non-missing)\n",
    "    non_missing_mask_X = ~np.isnan(X)[:, np.newaxis, :]\n",
    "    non_missing_mask_Y = ~np.isnan(Y)[np.newaxis, :, :]\n",
    "    \n",
    "    # Only consider differences where both corresponding values are non-missing\n",
    "    valid_diffs_mask = non_missing_mask_X & non_missing_mask_Y\n",
    "    valid_diffs = np.where(valid_diffs_mask, diffs, 0)\n",
    "    \n",
    "    # Compute squared differences\n",
    "    squared_diffs = valid_diffs ** 2\n",
    "    \n",
    "    # Count valid (non-missing) comparisons for normalization\n",
    "    valid_counts = np.sum(valid_diffs_mask, axis=2)\n",
    "    \n",
    "    # Avoid division by zero for rows/columns with all missing values\n",
    "    valid_counts[valid_counts == 0] = 1\n",
    "    \n",
    "    # Compute gamma as the inverse of valid comparisons\n",
    "    gamma = 1 / valid_counts\n",
    "    \n",
    "    # Sum squared differences along the feature dimension\n",
    "    sum_squared_diffs = np.sum(squared_diffs, axis=2)\n",
    "    \n",
    "    # Compute RBF kernel values\n",
    "    M = np.exp(-gamma * sum_squared_diffs)\n",
    "    \n",
    "    return M\n",
    "\n",
    "def penalize_wrong(y, y_pred, penalty):\n",
    "    \"Penalizes wrong guesses more, determined by the value of k\"\n",
    "    return np.mean(((y_pred - y)**2)*(1+penalty*(np.sign(y_pred)\n",
    "                                               != np.sign(y))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optima_model(model, param_dict, X, y, use_missing = True, impute = False):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = (X.loc[X['year'] < 2022, :], X.loc[X['year'] == 2022, :], \n",
    "                                        y.loc[X['year'] < 2022], y.loc[X['year'] == 2022])\n",
    "    \n",
    "\n",
    "    # Create fold structure so we can make a custom cross-validation for time-series\n",
    "    folds = [\n",
    "        (range(2002, 2006, 2), [2006, 2008]),\n",
    "        (range(2002, 2010, 2), [2010, 2012]),\n",
    "        (range(2002, 2014, 2), [2014, 2016]),\n",
    "        (range(2002, 2018, 2), [2018, 2020])\n",
    "    ]\n",
    "\n",
    "    cv = CustomTimeSeriesCV(folds)\n",
    "        \n",
    "    #Categorical features that need to be one-hot encoded    \n",
    "    one_hot_fts = ['office_type', 'open_seat', 'special', 'isMidterm']\n",
    "    \n",
    "    #These features we use regardless of whether or not we're using XGBoost or another model that can natively handle missing values\n",
    "    pass_fts_no_missing = ['incumbent_differential', 'prev_gen_margin', 'prev2_gen_margin',\n",
    "    'prev_dem_gen_tp', 'mean_specials_differential', 'pvi', 'weighted_genpoll', \n",
    "    'unweighted_genpoll', 'genballot_predicted_margin', 'specials_predicted_margin',\n",
    "    'house_chamber_margin', 'senate_chamber_margin', 'previous_cci',\n",
    "    'current_cci', 'change_cci', 'previous_gas', 'current_gas',\n",
    "    'change_gas', 'previous_unemployment', 'current_unemployment',\n",
    "    'change_unemployment', 'white_pct',\n",
    "    'black_pct', 'asian_pct', 'impoverished_pct', 'median_age', 'renting_pct', 'inflation']\n",
    "    \n",
    "    missing_fts = ['receipts_DEM', 'receipts_REP',\n",
    "       'disbursements_DEM', 'disbursements_REP', 'receipts_ratio', 'disbursements_ratio', \n",
    "       'unconvinced_pct', 'valid_weighted_ba', 'phone_unweighted_ba', 'online_unweighted_ba',\n",
    "       'all_unweighted_ba', 'all_unweighted', 'num_polls']\n",
    "    \n",
    "    #If we are able to use the missing values, we add them to the list of features\n",
    "    if use_missing:\n",
    "        pass_fts = pass_fts_no_missing + missing_fts\n",
    "    else:\n",
    "        pass_fts = pass_fts_no_missing \n",
    "        \n",
    "    #These features have <1% missing so we impute (all missing is from DC data or one race in PA) \n",
    "    simple_impute_features = [\n",
    "        \"absenteeexcusereq\", \"pollhours\", \"avgpollhours\",\n",
    "        \"minpollhours\", \"regdeadlines\", \"voteridlaws\", \"novoterid\",\n",
    "        \"noallmailvote\", \"noearlyvote\", \"nofelonreg\", \"nofelonsregafterincar\",\n",
    "        \"nonstrictid\", \"nonstrictphoto\", \"nopollplacereg\", \"nopr\",\n",
    "        \"nosamedayreg\", \"nostateholiday\", \"pr16\", \"pr17\",\n",
    "        \"pr175\", \"pr60\", \"pr90\", \"strictid\",\n",
    "        \"strictphoto\", \"covi_num\", \"median_income\"\n",
    "    ]     \n",
    "    \n",
    "    #Combine all the features\n",
    "    if impute: \n",
    "        preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts), \n",
    "        ('simple_impute', SimpleImputer(strategy='median'), simple_impute_features),\n",
    "        ('iterative_impute', IterativeImputer(max_iter=50), missing_fts),\n",
    "        #('num', 'passthrough', pass_fts_no_missing)\n",
    "        ])\n",
    "    else:\n",
    "        preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts), \n",
    "        ('impute', SimpleImputer(strategy='median'), simple_impute_features), \n",
    "        ('num', 'passthrough', missing_fts)\n",
    "        ])\n",
    "        \n",
    "    \n",
    "    def objective(params):\n",
    "        reg = model(**params)\n",
    "        pipe = Pipeline(steps = [\n",
    "            ('preprocessing', preprocessor), \n",
    "            ('scaling', StandardScaler()),\n",
    "            ('polynomial', PolynomialFeatures(degree=2)),\n",
    "            ('model', reg)])\n",
    "        \n",
    "        accuracies = []\n",
    "        training_accuracies = []\n",
    "        for train_idx, test_idx in cv.split(X_train):\n",
    "            pipe.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "            \n",
    "            predictions = pipe.predict(X_train.iloc[test_idx])\n",
    "            training_preds = pipe.predict(X_train.iloc[train_idx])\n",
    "            mae = median_absolute_error(y_train.iloc[test_idx], predictions)\n",
    "            accuracies.append(accuracy_score(y_train.iloc[test_idx], predictions))\n",
    "            training_accuracies.append(accuracy_score(y_train.iloc[train_idx], training_preds))\n",
    "        print(f\"Training accuracies: {training_accuracies}\")\n",
    "        print(f\"Validation accuracies: {accuracies}\")\n",
    "        print(f\"Overfitting differential: {np.mean(training_accuracies) - np.mean(accuracies)}\")\n",
    "        return {'loss': -1*np.mean(accuracies), 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "    trials = Trials()\n",
    "    best_params = fmin(fn=objective,\n",
    "                    space=param_dict,\n",
    "                    algo=tpe.suggest,\n",
    "                    trials=trials,\n",
    "                    early_stop_fn=no_progress_loss(iteration_stop_count=2))\n",
    "\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    acc_model = model(**best_params)\n",
    "    pipe = Pipeline(steps = [\n",
    "            ('preprocessing', preprocessor), \n",
    "            ('scaling', StandardScaler()), \n",
    "            ('model', acc_model)])\n",
    "    \n",
    "    \n",
    "    preprocessor.fit(X_train)\n",
    "    preprocessed_X_train = preprocessor.transform(X_train)\n",
    "    scaled_X_train = StandardScaler().fit_transform(preprocessed_X_train)\n",
    "    \n",
    "    acc_model.fit(scaled_X_train, y_train)\n",
    "    #Explainer = shap.Explainer(acc_model)\n",
    "    Explainer = shap.TreeExplainer(acc_model)\n",
    "    shap_values = Explainer.shap_values(scaled_X_train)\n",
    "    shap.summary_plot(shap_values, pd.DataFrame(data = scaled_X_train, columns = preprocessor.get_feature_names_out()), plot_type=\"bar\")    \n",
    "    return pd.DataFrame(shap_values, columns = preprocessor.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../cleaned_data/Engineered Dataset.csv\")\n",
    "\n",
    "\n",
    "X = data.drop(columns=['margin'])\n",
    "y = data['margin']\n",
    "y = y.where(y > 0, 0)\n",
    "y = y.where(y <= 0, 1)\n",
    "\n",
    "\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': hp.randint('n_estimators', 5, 300),\n",
    "    'max_depth': hp.randint('max_depth', 10, 100),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.2)),\n",
    "    'subsample': hp.uniform('subsample', 0., 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1),\n",
    "    'min_child_weight': hp.randint('min_child_weight', 1, 20),\n",
    "    'gamma': hp.loguniform('gamma', np.log(5), np.log(20)),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.01), np.log(100)),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(0.01), np.log(100))\n",
    "}\n",
    "\n",
    "\n",
    "xgb = xgboost.XGBClassifier\n",
    "#shaps = optima_model(xgb, param_dist_xgb, X, y, False)\n",
    "\n",
    "\n",
    "param_dist_svc = {\n",
    "    'C': hp.loguniform('C', -5, 5),\n",
    "    'kernel': missing_kernel,\n",
    "    'degree': hp.randint('degree', 1, 10)\n",
    "}\n",
    "\n",
    "svc = SVC\n",
    "#optima_model(svc, param_dist_svc, X, y, True, False)\n",
    "\n",
    "param_dist_logistic = {\n",
    "    'C': hp.loguniform('C', -5, 5),\n",
    "    'penalty': 'elasticnet',\n",
    "    'solver': 'saga',\n",
    "    'l1_ratio': hp.uniform('l1_ratio', 0, 1),\n",
    "    'max_iter': 100000,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "logistic = LogisticRegression\n",
    "#optima_model(logistic, param_dist_logistic, X, y, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MissForest.fit() got an unexpected keyword argument 'max_iter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m imputation_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincumbent_differential\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_gen_margin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev2_gen_margin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_dem_gen_tp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_specials_differential\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpvi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted_genpoll\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munweighted_genpoll\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenballot_predicted_margin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecials_predicted_margin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munconvinced_pct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_weighted_ba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphone_unweighted_ba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monline_unweighted_ba\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     11\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_unweighted_ba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_unweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_polls\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m mf \u001b[38;5;241m=\u001b[39m MissForest()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2006\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimputation_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m mf\u001b[38;5;241m.\u001b[39mtransform(X[X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2006\u001b[39m][[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincumbent_differential\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_gen_margin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev2_gen_margin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_dem_gen_tp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_specials_differential\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpvi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted_genpoll\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munweighted_genpoll\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenballot_predicted_margin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecials_predicted_margin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munconvinced_pct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_weighted_ba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphone_unweighted_ba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monline_unweighted_ba\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     27\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_unweighted_ba\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_unweighted\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_polls\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "\u001b[0;31mTypeError\u001b[0m: MissForest.fit() got an unexpected keyword argument 'max_iter'"
     ]
    }
   ],
   "source": [
    "imputation_columns = ['incumbent_differential', 'prev_gen_margin', 'prev2_gen_margin',\n",
    "    'prev_dem_gen_tp', 'mean_specials_differential', 'pvi', 'weighted_genpoll', \n",
    "    'unweighted_genpoll', 'genballot_predicted_margin', 'specials_predicted_margin',\n",
    "    'house_chamber_margin', 'senate_chamber_margin', 'previous_cci',\n",
    "    'current_cci', 'change_cci', 'previous_gas', 'current_gas',\n",
    "    'change_gas', 'previous_unemployment', 'current_unemployment',\n",
    "    'change_unemployment', 'white_pct',\n",
    "    'black_pct', 'asian_pct', 'impoverished_pct', 'median_age', 'renting_pct', 'inflation','receipts_DEM', 'receipts_REP',\n",
    "       'disbursements_DEM', 'disbursements_REP', 'receipts_ratio', 'disbursements_ratio', \n",
    "       'unconvinced_pct', 'valid_weighted_ba', 'phone_unweighted_ba', 'online_unweighted_ba',\n",
    "       'all_unweighted_ba', 'all_unweighted', 'num_polls']\n",
    "\n",
    "mf = MissForest()\n",
    "\n",
    "mf.fit(X[X['year'] < 2006][imputation_columns])\n",
    "data_pre_2006 = mf.transform(X[X['year'] < 2006][imputation_columns])\n",
    "data_2006 = mf.transform(X[X['year'] == 2006][imputation_columns])\n",
    "mf.fit(X[X['year'] < 2010][imputation_columns])\n",
    "data_2008 = mf.transform(X[X['year'] == 2008][imputation_columns])\n",
    "data_2010 = mf.transform(X[X['year'] == 2010][imputation_columns])\n",
    "mf.fit(X[X['year'] < 2014][imputation_columns])\n",
    "data_2012 = mf.transform(X[X['year'] == 2012][imputation_columns])\n",
    "data_2014 = mf.transform(X[X['year'] == 2014][imputation_columns])\n",
    "mf.fit(X[X['year'] < 2018][imputation_columns])\n",
    "data_2016 = mf.transform(X[X['year'] == 2016][imputation_columns])\n",
    "data_2018 = mf.transform(X[X['year'] == 2018][imputation_columns])\n",
    "mf.fit(X[X['year'] < 2022][imputation_columns])\n",
    "data_2020 = mf.transform(X[X['year'] == 2020][imputation_columns])\n",
    "data_2022 = mf.transform(X[X['year'] == 2022][imputation_columns])\n",
    "mf.fit(X[X['year'] < 2024][imputation_columns])\n",
    "data_2024 = mf.transform(X[X['year'] == 2024][imputation_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>2004</td>\n",
       "      <td>WA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5388</th>\n",
       "      <td>2004</td>\n",
       "      <td>WI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5389</th>\n",
       "      <td>2004</td>\n",
       "      <td>WV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>2004</td>\n",
       "      <td>WY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591</th>\n",
       "      <td>2004</td>\n",
       "      <td>AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1328 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year state\n",
       "0     2002    AK\n",
       "1     2002    AL\n",
       "2     2002    AL\n",
       "3     2002    AL\n",
       "4     2002    AL\n",
       "...    ...   ...\n",
       "5387  2004    WA\n",
       "5388  2004    WI\n",
       "5389  2004    WV\n",
       "5390  2004    WY\n",
       "5591  2004    AK\n",
       "\n",
       "[1328 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X['year'] <= 2006][['year', 'state']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
