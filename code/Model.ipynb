{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, BaseCrossValidator\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, max_error, accuracy_score, median_absolute_error, make_scorer\n",
    "import xgboost\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from hyperopt.pyll.base import Apply\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "from missforest.missforest import MissForest\n",
    "\n",
    "#Creating a custom time series cross-validator\n",
    "class CustomTimeSeriesCV(BaseCrossValidator):\n",
    "    \"\"\"Creates an iterator that contains the indices from each dataset based on the years given\"\"\"\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for train_years, test_years in self.years:\n",
    "            train_indices = np.where(X['year'].isin(train_years))[0]\n",
    "            test_indices = np.where(X['year'].isin(test_years))[0]\n",
    "            yield train_indices, test_indices\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return len(self.years)\n",
    "\n",
    "class MissForestImputer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Probably useless class -- theoretically uses MissForest with sklearn's API, but it's not working as expected.\n",
    "    For some reason, LightGBM isn't able to create trees on the missing data\"\"\"\n",
    "    def __init__(self, max_iter = 20):\n",
    "        self.model = MissForest(max_iter = max_iter)\n",
    "    \n",
    "    def get_params(self, deep: bool = False) -> dict:\n",
    "        return {'max_iter': self.model.max_iter}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X)\n",
    "        return self  # Return self to enable chaining\n",
    "\n",
    "    def transform(self, X):\n",
    "        # MissForest's transform method is actually predict in most cases\n",
    "        X_imputed = self.model.transform(X)\n",
    "        return X_imputed\n",
    "\n",
    "    def fit_transform(self, X, y=None, **fit_params):\n",
    "        return self.model.fit_transform(X)\n",
    " \n",
    "    \n",
    "def missing_kernel(X, Y):\n",
    "    \"\"\"Kernel method that works with missing values in the data. It computes the RBF kernel between two matrices X and Y.\n",
    "    Theoretically, it should work for sklearn's SVM, but I can't get it to work with gamma/C parameters.\"\"\"\n",
    "    # Calculate pairwise squared differences using broadcasting, with an extra dimension for vectorization\n",
    "    diffs = X[:, np.newaxis, :] - Y[np.newaxis, :, :]\n",
    "    \n",
    "    # Create masks for non-missing values (True for non-missing)\n",
    "    non_missing_mask_X = ~np.isnan(X)[:, np.newaxis, :]\n",
    "    non_missing_mask_Y = ~np.isnan(Y)[np.newaxis, :, :]\n",
    "    \n",
    "    # Only consider differences where both corresponding values are non-missing\n",
    "    valid_diffs_mask = non_missing_mask_X & non_missing_mask_Y\n",
    "    valid_diffs = np.where(valid_diffs_mask, diffs, 0)\n",
    "    \n",
    "    # Compute squared differences\n",
    "    squared_diffs = valid_diffs ** 2\n",
    "    \n",
    "    # Count valid (non-missing) comparisons for normalization\n",
    "    valid_counts = np.sum(valid_diffs_mask, axis=2)\n",
    "    \n",
    "    # Avoid division by zero for rows/columns with all missing values\n",
    "    valid_counts[valid_counts == 0] = 1\n",
    "    \n",
    "    # Compute gamma as the inverse of valid comparisons\n",
    "    gamma = 1 / valid_counts\n",
    "    \n",
    "    # Sum squared differences along the feature dimension\n",
    "    sum_squared_diffs = np.sum(squared_diffs, axis=2)\n",
    "    \n",
    "    # Compute RBF kernel values\n",
    "    M = np.exp(-gamma * sum_squared_diffs)\n",
    "    \n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optima_model(model, param_dict, X, y, classify = True):\n",
    "    \"\"\"Performs hyperparameter optimization for a a given model, keeping track of loss. \n",
    "    ## Parameters:\n",
    "    model: sklearnable model, like XGBoost or Linreg\n",
    "    param_dict: dictionary of hyperparameters to optimize\n",
    "    X: DataFrame with features\n",
    "    y: Series with target variable\n",
    "    classify: Boolean, whether we are using a classifier or regressor\"\"\"\n",
    "    \n",
    "    if classify:\n",
    "        #When classifying, y must be boolean\n",
    "        y = y.where(y > 0, 0)\n",
    "        y = y.where(y <= 0, 1)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = (X.loc[X['year'] < 2022, :], X.loc[X['year'] == 2022, :], \n",
    "                                        y.loc[X['year'] < 2022], y.loc[X['year'] == 2022])\n",
    "    \n",
    "\n",
    "    # Create fold structure so we can make a custom cross-validation for time-series\n",
    "    folds = [\n",
    "        (range(2002, 2006, 2), [2006, 2008]),\n",
    "        (range(2002, 2010, 2), [2010, 2012]),\n",
    "        (range(2002, 2014, 2), [2014, 2016]),\n",
    "        (range(2002, 2018, 2), [2018, 2020])\n",
    "    ]\n",
    "\n",
    "    cv = CustomTimeSeriesCV(folds)\n",
    "        \n",
    "    #Categorical features that need to be one-hot encoded    \n",
    "    one_hot_fts = ['office_type', 'open_seat', 'special', 'isMidterm']\n",
    "    \n",
    "    #Cont features that should be pass-throughed (and later scaled)\n",
    "    cont_fts = ['incumbent_differential', 'prev_gen_margin', 'prev2_gen_margin',\n",
    "    'prev_dem_gen_tp', 'mean_specials_differential', 'pvi', 'weighted_genpoll', \n",
    "    'unweighted_genpoll', 'genballot_predicted_margin', 'specials_predicted_margin',\n",
    "    'house_chamber_margin', 'senate_chamber_margin', 'previous_cci',\n",
    "    'current_cci', 'change_cci', 'previous_gas', 'current_gas',\n",
    "    'change_gas', 'previous_unemployment', 'current_unemployment',\n",
    "    'change_unemployment', 'white_pct', 'black_pct', 'asian_pct', 'impoverished_pct', \n",
    "    'median_age', 'renting_pct', 'inflation', 'receipts_DEM', 'receipts_REP',\n",
    "    'disbursements_DEM', 'disbursements_REP', 'receipts_ratio', 'disbursements_ratio', \n",
    "    'unconvinced_pct', 'valid_weighted_ba', 'phone_unweighted_ba', 'online_unweighted_ba',\n",
    "    'all_unweighted_ba', 'all_unweighted', 'num_polls', \n",
    "    \"absenteeexcusereq\", \"pollhours\", \"avgpollhours\",\n",
    "    \"minpollhours\", \"regdeadlines\", \"voteridlaws\", \"novoterid\",\n",
    "    \"noallmailvote\", \"noearlyvote\", \"nofelonreg\", \"nofelonsregafterincar\",\n",
    "    \"nonstrictid\", \"nonstrictphoto\", \"nopollplacereg\", \"nopr\",\n",
    "    \"nosamedayreg\", \"nostateholiday\", \"pr16\", \"pr17\",\n",
    "    \"pr175\", \"pr60\", \"pr90\", \"strictid\",\n",
    "    \"strictphoto\", \"covi_num\", \"median_income\"]\n",
    "        \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts),\n",
    "        ('num', 'passthrough', cont_fts)])\n",
    "    \n",
    "    \n",
    "    def objective(params):\n",
    "        \"Function that takes in hyperparameters and returns loss, that Hyperopt will minimize.\"\n",
    "        reg = model(**params)\n",
    "        pipe = Pipeline(steps = [\n",
    "            ('preprocessing', preprocessor), \n",
    "            ('scaling', StandardScaler()),\n",
    "            #('polynomial', PolynomialFeatures(degree=2)), Use only if you want to add polynomial features\n",
    "            ('model', reg)])\n",
    "        \n",
    "        \n",
    "        training_loss = []\n",
    "        testing_loss = []\n",
    "        for train_idx, test_idx in cv.split(X_train):\n",
    "            \"\"\"Goes through each fold and calculates loss.\n",
    "            Note: We use median absolute error because it is more robust to outliers than mean absolute error.\n",
    "            We also expect earlier folds to have higher error, since they have less data to train on.\"\"\"\n",
    "            pipe.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "            \n",
    "            predictions = pipe.predict(X_train.iloc[test_idx])\n",
    "            training_preds = pipe.predict(X_train.iloc[train_idx])\n",
    "            if classify:\n",
    "                \"We multiply by -1 because hyperopt minimizes the loss, but we want to maximize accuracy\"\n",
    "                testing_loss.append(-1 * accuracy_score(y_train.iloc[test_idx], predictions))\n",
    "                training_loss.append(-1 * accuracy_score(y_train.iloc[train_idx], training_preds))\n",
    "            else:\n",
    "                testing_loss.append(median_absolute_error(y_train.iloc[test_idx], predictions))\n",
    "                training_loss.append(median_absolute_error(y_train.iloc[train_idx], training_preds))\n",
    "            \n",
    "        print(f\"Training loss: {training_loss}\")\n",
    "        print(f\"Validation loss: {testing_loss}\")\n",
    "        print(f\"Overfitting differential: {np.mean(testing_loss) - np.mean(training_loss)}\")\n",
    "        return {'loss': np.mean(testing_loss), 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "    \"Hyperopt uses the TPE algorithm to optimize hyperparameters. We use the no_progress_loss function to stop early if we don't see progress.\"\n",
    "    best_params = fmin(fn=objective,\n",
    "                    space=param_dict,\n",
    "                    algo=tpe.suggest,\n",
    "                    trials=Trials(),\n",
    "                    early_stop_fn=no_progress_loss(iteration_stop_count=10))\n",
    "\n",
    "    print(\"Best parameters:\", best_params)\n",
    "    \n",
    "    acc_model = model(**best_params)\n",
    "    \n",
    "    \"This works with SHAP to calculate importance values. Shap can't handle pipelines, so we need to preprocess the data first.\"\n",
    "    preprocessed_X_train = preprocessor.fit_transform(X_train)\n",
    "    scaled_X_train = StandardScaler().fit_transform(preprocessed_X_train)\n",
    "    \n",
    "    acc_model.fit(scaled_X_train, y_train)\n",
    "    \"Change this to TreeExplainer if you are using a tree-based model\"\n",
    "    #Explainer = shap.Explainer(acc_model)\n",
    "    Explainer = shap.TreeExplainer(acc_model)\n",
    "    shap_values = Explainer.shap_values(scaled_X_train)\n",
    "    shap.summary_plot(shap_values, pd.DataFrame(data = scaled_X_train, columns = preprocessor.get_feature_names_out()), plot_type=\"bar\") \n",
    "    \"Returns a dataframe with SHAP values for future analysis\"   \n",
    "    return pd.DataFrame(shap_values, columns = preprocessor.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\none_hot_fts = ['office_type', 'open_seat', 'special', 'isMidterm', 'state', 'year', 'district', 'final_rating']\\n\\nimputing_columns = [x for x in X.columns if x not in one_hot_fts]\\nbi = BiScaler()\\nbi.fit(np.array(X[X['year'] < 2006][imputing_columns]))\\ndata_pre_2006 = bi.transform(np.array(X[X['year'] < 2006][imputing_columns]))\\ndata_2006 = bi.transform(np.array(X[X['year'] == 2006][imputing_columns]))\\nbi.fit(np.array(X[X['year'] < 2010][imputing_columns]))\\ndata_2008 = bi.transform(np.array(X[X['year'] == 2008][imputing_columns]))\\ndata_2010 = bi.transform(np.array(X[X['year'] == 2010][imputing_columns]))\\nbi.fit(np.array(X[X['year'] < 2014][imputing_columns]))\\ndata_2012 = bi.transform(np.array(X[X['year'] == 2012][imputing_columns]))\\ndata_2014 = bi.transform(np.array(X[X['year'] == 2014][imputing_columns]))\\nbi.fit(np.array(X[X['year'] < 2018][imputing_columns]))\\ndata_2016 = bi.transform(np.array(X[X['year'] == 2016][imputing_columns]))\\ndata_2018 = bi.transform(np.array(X[X['year'] == 2018][imputing_columns]))\\nbi.fit(np.array(X[X['year'] < 2022][imputing_columns]))\\ndata_2020 = bi.transform(np.array(X[X['year'] == 2020][imputing_columns]))\\ndata_2022 = bi.transform(np.array(X[X['year'] == 2022][imputing_columns]))\\nbi.fit(np.array(X[X['year'] < 2024][imputing_columns]))\\ndata_2024 = bi.transform(np.array(X[X['year'] == 2024][imputing_columns]))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "#methods to impiute, but can't get any to work right now...\n",
    "from fancyimpute import KNN, NuclearNormMinimization, SoftImpute, BiScaler\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"../cleaned_data/Engineered Dataset.csv\")\n",
    "data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "X = data.drop(columns=['margin'])\n",
    "y = data['margin']\n",
    "\n",
    "\n",
    "#This imputation method doesn't work :(\n",
    "#We want to impute BEFORE we actually run the data through the model, being sure to not leak data beforehand\n",
    "\"\"\"\n",
    "one_hot_fts = ['office_type', 'open_seat', 'special', 'isMidterm', 'state', 'year', 'district', 'final_rating']\n",
    "\n",
    "imputing_columns = [x for x in X.columns if x not in one_hot_fts]\n",
    "bi = BiScaler()\n",
    "bi.fit(np.array(X[X['year'] < 2006][imputing_columns]))\n",
    "data_pre_2006 = bi.transform(np.array(X[X['year'] < 2006][imputing_columns]))\n",
    "data_2006 = bi.transform(np.array(X[X['year'] == 2006][imputing_columns]))\n",
    "bi.fit(np.array(X[X['year'] < 2010][imputing_columns]))\n",
    "data_2008 = bi.transform(np.array(X[X['year'] == 2008][imputing_columns]))\n",
    "data_2010 = bi.transform(np.array(X[X['year'] == 2010][imputing_columns]))\n",
    "bi.fit(np.array(X[X['year'] < 2014][imputing_columns]))\n",
    "data_2012 = bi.transform(np.array(X[X['year'] == 2012][imputing_columns]))\n",
    "data_2014 = bi.transform(np.array(X[X['year'] == 2014][imputing_columns]))\n",
    "bi.fit(np.array(X[X['year'] < 2018][imputing_columns]))\n",
    "data_2016 = bi.transform(np.array(X[X['year'] == 2016][imputing_columns]))\n",
    "data_2018 = bi.transform(np.array(X[X['year'] == 2018][imputing_columns]))\n",
    "bi.fit(np.array(X[X['year'] < 2022][imputing_columns]))\n",
    "data_2020 = bi.transform(np.array(X[X['year'] == 2020][imputing_columns]))\n",
    "data_2022 = bi.transform(np.array(X[X['year'] == 2022][imputing_columns]))\n",
    "bi.fit(np.array(X[X['year'] < 2024][imputing_columns]))\n",
    "data_2024 = bi.transform(np.array(X[X['year'] == 2024][imputing_columns]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running xgb with hyperopt\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': hp.randint('n_estimators', 5, 300),\n",
    "    'max_depth': hp.randint('max_depth', 10, 100),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.2)),\n",
    "    'subsample': hp.uniform('subsample', 0., 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1),\n",
    "    'min_child_weight': hp.randint('min_child_weight', 1, 20),\n",
    "    'gamma': hp.loguniform('gamma', np.log(5), np.log(20)),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', np.log(0.01), np.log(100)),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', np.log(0.01), np.log(100))\n",
    "}\n",
    "\n",
    "\n",
    "xgb = xgboost.XGBClassifier\n",
    "optima_model(xgb, param_dist_xgb, X, y, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
