{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import shap\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV, BaseCrossValidator\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, max_error, accuracy_score, median_absolute_error, make_scorer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from hyperopt.pyll.base import Apply\n",
    "\n",
    "\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "from hpsklearn import HyperoptEstimator, any_regressor, any_preprocessing\n",
    "from hyperopt import tpe\n",
    "\n",
    "def incorrect_penalizer(y_true, y_pred, multiplier=1):\n",
    "    \"\"\"\n",
    "    Scorer function where wrong predictions are penalized more than correct predictions.\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Calculate whether the signs of the true and predicted values are different\n",
    "    sign_diff = np.sign(y_true) != np.sign(y_pred)\n",
    "    \n",
    "    # Compute absolute errors\n",
    "    abs_errors = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Multiply the error where the signs are different\n",
    "    penalized_errors = abs_errors * (1 + (multiplier - 1) * sign_diff.astype(int))\n",
    "    \n",
    "    return np.mean(penalized_errors)\n",
    "\n",
    "#Creating a custom time series cross-validator\n",
    "class CustomTimeSeriesCV(BaseCrossValidator):\n",
    "    \"\"\"Creates an iterator that contains the indices from each dataset based on the years given\"\"\"\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for train_years, test_years in self.years:\n",
    "            train_indices = np.where(X['year'].isin(train_years))[0]\n",
    "            test_indices = np.where(X['year'].isin(test_years))[0]\n",
    "            yield train_indices, test_indices\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return len(self.years)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Time Series Cross-Validator -- Every year has a different number of races, so we can't use the regular time series CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model(model, param_dict, X, y, iterations = 50):\n",
    "   \"Creating a custom CV to look through errors in a detailed manner\"\n",
    "   X_other, X_test, y_other, y_test = (X.loc[X['year'] < 2022, :], X.loc[X['year'] == 2022, :], \n",
    "                                    y.loc[X['year'] < 2022], y.loc[X['year'] == 2022])\n",
    "\n",
    "   models = []\n",
    "   test_scores = []\n",
    "   for random_state in range(1, iterations):\n",
    "      \n",
    "      one_hot_fts = ['office_type', 'final_rating', 'open_seat']\n",
    "      std_fts = ['midterm', 'incumbent_margin', 'covi_num','special', 'prev_gb_margin', 'prev2_gb_margin',\n",
    "         'mean_specials_differential', 'pvi', 'previous_cci', 'current_cci',\n",
    "         'previous_gas', 'current_gas',  'previous_unemployment',\n",
    "         'current_unemployment', 'absenteeexcusereq', 'pollhours',\n",
    "         'avgpollhours', 'maxpollhours', 'minpollhours', 'regdeadlines',\n",
    "         'voteridlaws', 'novoterid', 'noallmailvote', 'noearlyvote',\n",
    "         'nofelonreg', 'nofelonsregafterincar', 'nonstrictid', 'nonstrictphoto',\n",
    "         'noonlineregistration', 'nopermanentabsentee', 'nopollplacereg', 'nopr',\n",
    "         'nosamedayreg', 'nostateholiday', 'pr16', 'pr17', 'pr175', 'pr60',\n",
    "         'pr90', 'strictid', 'strictphoto', 'house_chamber_margin',\n",
    "         'senate_chamber_margin', 'change_cci', 'change_unemployment']\n",
    "         \n",
    "      preprocessor = ColumnTransformer([\n",
    "      ('cat', OneHotEncoder(), one_hot_fts), \n",
    "      ('num', 'passthrough', std_fts)])\n",
    "      \n",
    "      parameters = {key: value.rvs(random_state=random_state) for key, value in param_dict.items()}\n",
    "      model = model.set_params(**parameters)\n",
    "      \n",
    "      pipe = make_pipeline(preprocessor, model)\n",
    "      models.append(pipe)\n",
    "      \n",
    "      folds = [(range(2002, 2006, 2), [2006, 2008]),\n",
    "         (range(2002, 2010, 2), [2010, 2012]),\n",
    "         (range(2002, 2014, 2), [2014, 2016]),\n",
    "         (range(2002, 2018, 2), [2018, 2020])]\n",
    "      fold_scores = []\n",
    "      for train, test in folds:\n",
    "            X_train, X_val = X_other.loc[X['year'].isin(train), :], X_other.loc[X['year'].isin(test), :]\n",
    "            y_train, y_val = y_other[X_other['year'].isin(train)], y_other[X_other['year'].isin(test)]\n",
    "            \n",
    "            pipe.fit(X_train, y_train)\n",
    "            y_pred = pipe.predict(X_val)\n",
    "            fold_scores.append(median_absolute_error(y_val, y_pred))\n",
    "            print(f'Mean absolute error for {train} is {mean_absolute_error(y_val, y_pred)}')\n",
    "            print(f'Mean squared error for {train} is {mean_squared_error(y_val, y_pred)}')\n",
    "            print(f'Max error for {train} is {max_error(y_val, y_pred)}')\n",
    "            print(f'Median absolute error for {train} is {median_absolute_error(y_val, y_pred)}')\n",
    "            print('---------------------------------------------------------')\n",
    "      test_scores.append(np.mean(fold_scores))\n",
    "      \n",
    "   val_score = test_scores[np.argmin(test_scores)]\n",
    "   best_model = models[np.argmin(test_scores)]\n",
    "   best_model.fit(X_other, y_other)\n",
    "\n",
    "   print(f\"training score is {mean_absolute_error(y_other, best_model.predict(X_other))}\")\n",
    "   print(f\"validation score is {val_score}\")\n",
    "   print(f\"test_score is {mean_absolute_error(y_test, best_model.predict(X_test))}\")\n",
    "   return best_model, val_score   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../cleaned_data/Finalized Dataset.csv\")\n",
    "filtered_data = data.drop(columns = ['district']).assign(pvi = lambda x: x['pvi'] * 2, \n",
    "                                                         midterm = lambda x: x['year'] % 4 != 0) \n",
    "                                                        \n",
    "X = filtered_data.drop(columns=['margin'])\n",
    "y = filtered_data['margin']\n",
    "\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': randint(10, 251),  # Discrete uniform distribution\n",
    "    'max_depth': randint(3, 16),  # Discrete uniform distribution\n",
    "    'learning_rate': uniform(0.001, 0.199),  # Continuous uniform distribution\n",
    "    'subsample': uniform(0.3, 0.7),  # Continuous uniform distribution\n",
    "    'colsample_bytree': uniform(0.3, 0.7),  # Continuous uniform distribution\n",
    "    'min_child_weight': randint(5, 16),  # Discrete uniform distribution\n",
    "    'gamma': uniform(0.01, 99.99),  # Continuous uniform distribution\n",
    "    'reg_alpha': uniform(0.01, 99.99),  # Continuous uniform distribution\n",
    "    'reg_lambda': uniform(0.01, 99.99)  # Continuous uniform distribution\n",
    "}\n",
    "\n",
    "#xgb = xgboost.XGBRegressor(n_jobs = -1)\n",
    "#custom_model(xgb, param_dist_xgb, X, y, iterations = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, param_dict, X, y, iterations = 75):\n",
    "    \"\"\"Runs through a given model to get the best estimator of that model, as well as the train/test score values.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = (X.loc[X['year'] < 2022, :], X.loc[X['year'] == 2022, :], \n",
    "                                        y.loc[X['year'] < 2022], y.loc[X['year'] == 2022])\n",
    "    \n",
    "    folds = [(range(2002, 2006, 2), [2006, 2008]),\n",
    "        (range(2002, 2010, 2), [2010, 2012]),\n",
    "        (range(2002, 2014, 2), [2014, 2016]),\n",
    "        (range(2002, 2018, 2), [2018, 2020])]\n",
    "\n",
    "    cv = CustomTimeSeriesCV(folds)\n",
    "        \n",
    "    one_hot_fts = ['office_type', 'final_rating', 'open_seat']\n",
    "    std_fts = ['midterm', 'incumbent_margin', 'covi_num','special', 'prev_gb_margin', 'prev2_gb_margin',\n",
    "       'mean_specials_differential', 'pvi', 'previous_cci', 'current_cci',\n",
    "       'previous_gas', 'current_gas',  'previous_unemployment',\n",
    "       'current_unemployment', 'absenteeexcusereq', 'pollhours',\n",
    "       'avgpollhours', 'maxpollhours', 'minpollhours', 'regdeadlines',\n",
    "       'voteridlaws', 'novoterid', 'noallmailvote', 'noearlyvote',\n",
    "       'nofelonreg', 'nofelonsregafterincar', 'nonstrictid', 'nonstrictphoto',\n",
    "       'noonlineregistration', 'nopermanentabsentee', 'nopollplacereg', 'nopr',\n",
    "       'nosamedayreg', 'nostateholiday', 'pr16', 'pr17', 'pr175', 'pr60',\n",
    "       'pr90', 'strictid', 'strictphoto', 'house_chamber_margin',\n",
    "       'senate_chamber_margin', 'change_cci', 'change_unemployment']\n",
    "        \n",
    "    preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(), one_hot_fts), \n",
    "    ('num', 'passthrough', std_fts)])\n",
    "    \n",
    "    model_name = model.__class__.__name__\n",
    "        \n",
    "    param_dict = {f\"{model_name.lower()}__{key}\": value for key, value in param_dict.items()}\n",
    "    \n",
    "    pipe = make_pipeline(preprocessor, model)\n",
    "    \n",
    "    bayes = RandomizedSearchCV(pipe, param_dict, n_iter=iterations, scoring='neg_median_absolute_error', cv = cv, verbose = 1)\n",
    "    #bayes = BayesSearchCV(pipe, param_dict, n_iter=iterations, scoring=custom_scorer, cv = cv, verbose = 1)\n",
    "    \n",
    "    np.int = int # bayes uses np.int, which is deprecated -- this removes the error!\n",
    "    bayes.fit(X_train, y_train)\n",
    "    \n",
    "    #Code only used if I want to debug and see how good the model is doing\n",
    "    train_score_mae = mean_absolute_error(y_train, bayes.predict(X_train))\n",
    "    train_score_mse = mean_squared_error(y_train, bayes.predict(X_train))\n",
    "    test_score_mae = mean_absolute_error(y_test, bayes.predict(X_test))\n",
    "    test_score_med_ae = median_absolute_error(y_test, bayes.predict(X_test))\n",
    "    pct_right = np.mean(np.sign(y_test) == np.sign(bayes.predict(X_test)))\n",
    "    max_test_score = max_error(y_test, bayes.predict(X_test))\n",
    "\n",
    "    print(f\"Train MAE is {train_score_mae}, Train MSE is {train_score_mse}\")\n",
    "    print(f\"Test MAE is {test_score_mae}, Test MedAE is {test_score_med_ae}\")\n",
    "    print(f\"Maximum error is {max_test_score}\")\n",
    "    print(f\"The pct of correct predictions is {pct_right}\")\n",
    "    print(f\"Score of estimator on non-2022 data is {-1*bayes.best_score_}\")\n",
    "    \n",
    "    return (bayes, train_score_mae, test_score_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 50 candidates, totalling 200 fits\n",
      "Train MAE is 4.947835442945541, Train MSE is 49.251908424160355\n",
      "Test MAE is 6.820283865911831, Test MedAE is 5.522969699239104\n",
      "Maximum error is 25.350468935011072\n",
      "The pct of correct predictions is 0.94\n",
      "Score of estimator on non-2022 data is 9.97431184110659\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../cleaned_data/Finalized Dataset.csv\")\n",
    "filtered_data = data.drop(columns = ['district']).assign(pvi = lambda x: x['pvi'] * 2, \n",
    "                                                         midterm = lambda x: x['year'] % 4 != 0)\n",
    "X = filtered_data.drop(columns=['margin'])\n",
    "y = filtered_data['margin']\n",
    "\n",
    "\"\"\"param_dict_xgb = {\n",
    "    'n_estimators': Integer(10, 250),\n",
    "    'max_depth': Integer(3, 15),  # Reduced from 15\n",
    "    'learning_rate': Real(0.001, 0.2, prior = 'log-uniform'),  # Reduced upper limit\n",
    "    'subsample': Real(0.3, 1, prior = 'uniform'),  # Decreased\n",
    "    'colsample_bytree': Real(0.3, 1, prior = 'uniform'),  # Decreased\n",
    "    'min_child_weight': Integer(5, 15),  # Increased lower limit\n",
    "    'gamma': Real(0.01, 100, prior = 'log-uniform'),  # Regularization\n",
    "    'reg_alpha': Real(0.01, 100, prior = 'log-uniform'),  # Regularization\n",
    "    'reg_lambda': Real(0.01, 100, prior = 'log-uniform')  # Regularization\n",
    "}\"\"\"\n",
    "\n",
    "\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': randint(10, 251),  # Discrete uniform distribution\n",
    "    'max_depth': randint(3, 16),  # Discrete uniform distribution\n",
    "    'learning_rate': uniform(0.001, 0.199),  # Continuous uniform distribution\n",
    "    'subsample': uniform(0.3, 0.7),  # Continuous uniform distribution\n",
    "    'colsample_bytree': uniform(0.3, 0.7),  # Continuous uniform distribution\n",
    "    'min_child_weight': randint(5, 16),  # Discrete uniform distribution\n",
    "    'gamma': uniform(0.01, 99.99),  # Continuous uniform distribution\n",
    "    'reg_alpha': uniform(0.01, 99.99),  # Continuous uniform distribution\n",
    "    'reg_lambda': uniform(0.01, 99.99)  # Continuous uniform distribution\n",
    "}\n",
    "\n",
    "xgb = xgboost.XGBRegressor(n_jobs = -1)\n",
    "(bayes_model, train_score, test_score) = run_model(xgb, param_dist_xgb, X, y, iterations = 50)\n",
    "\n",
    "\n",
    "param_dict_gbr = {\n",
    "    'loss': Categorical(['squared_error', 'absolute_error']), \n",
    "    'learning_rate': Real(0.01, 1, prior='log-uniform'), \n",
    "    'max_iter': Integer(10, 200), \n",
    "    'max_leaf_nodes': Integer(10, 100), \n",
    "    'max_depth': Integer(5, 100), \n",
    "    'min_samples_leaf': Integer(5, 100), \n",
    "    'l2_regularization': Real(0.001, 1000, prior='log-uniform'), \n",
    "    'interaction_cst': Categorical(['pairwise', 'no_interactions'])\n",
    "}\n",
    "\n",
    "gbr = HistGradientBoostingRegressor()\n",
    "\n",
    "\n",
    "param_dict_gbm = {\n",
    "    'boosting_type': Categorical(['dart']),  # Stick to traditional to reduce complexity.\n",
    "    'max_depth': Integer(2, 6),  # Lower max depth to control overfitting.\n",
    "    'num_leaves': Integer(2, 8),  # Lower number of leaves to control overfitting.\n",
    "    'learning_rate': Real(0.01, 0.1, prior='log-uniform'),  # Lower learning rates can lead to better generalization.\n",
    "    'min_data_in_leaf': Integer(20, 40),  # Increase to provide a more conservative approach.\n",
    "    'min_sum_hessian_in_leaf': Real(0.001, 0.1),  # Increasing this value can help with overfitting.\n",
    "    'n_estimators': Integer(100, 300),  # Reducing the upper limit to prevent overfitting.\n",
    "    'subsample_for_bin': Integer(20000, 200000),  # Adjust based on your data size and feature.\n",
    "    'class_weight': Categorical([None]),  # Unless you have imbalanced classes, stick to None.\n",
    "    'min_split_gain': Real(0.1, 1.0),  # Increase the minimum gain to reduce complex tree structures.\n",
    "    'min_child_weight': Real(0.01, 1),  # Increase to add more constraints on the tree.\n",
    "    'min_child_samples': Integer(20, 50),  # Increase to ensure more samples inform each split.\n",
    "    'subsample': Real(0.5, 0.8),  # Decrease to add more randomness and reduce overfitting.\n",
    "    'subsample_freq': Integer(1, 10),  # Ensure subsampling happens more regularly.\n",
    "    'colsample_bytree': Real(0.5, 0.8),  # Decrease to add more randomness and reduce overfitting.\n",
    "    'reg_alpha': Real(0.1, 10, prior='log-uniform'),  # Increase L1 regularization.\n",
    "    'reg_lambda': Real(0.1, 10, prior='log-uniform')  # Increase L2 regularization.\n",
    "}\n",
    "\n",
    "\n",
    "gbm = lightgbm.LGBMRegressor(n_jobs = -1)\n",
    "\n",
    "#run_model(gbr, param_dict_gbr, X, y, iterations=75)\n",
    "#run_model(gbm, param_dict_gbm, X, y, iterations=40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.133133176073309, 6.509490945257898, 8.27693674326877, 7.450457470262208]\n",
      "[9.018630506572016, 14.582932055926348, 20.772794212389055, 24.435695357558117]                                  \n",
      "[7.1021192763455385, 11.93936630018348, 16.372471657337062, 18.599882900339047]                                  \n",
      "[5.154632127337833, 9.261376521503752, 10.831056858367742, 13.218816728108353]                                   \n",
      "[9.464046167150084, 15.06618171029456, 21.63934676656595, 25.69906711780782]                                     \n",
      "  0%|          | 5/9223372036854775807 [00:07<4000616549456252:01:04,  1.56s/trial, best loss: 6.592504583715546]\n",
      "Best parameters: {'colsample_bytree': 0.4547867536333304, 'gamma': 12.010375674360388, 'learning_rate': 0.02202605484969771, 'max_depth': 5, 'min_child_weight': 9, 'n_estimators': 142, 'reg_alpha': 0.24043206714908386, 'reg_lambda': 0.2391480567734706, 'subsample': 0.7775845345223572}\n",
      "[4.569284647179207, 6.261917112804594, 8.43490559978272, 7.65320124596431]\n",
      "[3.980095247521941, 6.188243014314347, 8.199107516439343, 8.478145226420041]                                     \n",
      "[8.27750872627538, 13.211093089360803, 18.74120648348932, 21.78042138260297]                                     \n",
      "[6.887577762693979, 11.414087052004419, 16.144938922387404, 17.851351067979728]                                   \n",
      "[4.686480542545947, 7.430523693568928, 9.95116132392179, 9.211191053694755]                                       \n",
      "[4.656596068327356, 8.271333478709163, 10.190561837153476, 11.542623794750101]                                    \n",
      "[9.597223534549402, 15.555819211127105, 22.63641445494617, 26.370998310150775]                                    \n",
      "  0%|          | 7/9223372036854775807 [01:25<31234461472777976:02:08, 12.19s/trial, best loss: 6.711397751173918]\n",
      "Best parameters: {'colsample_bytree': 0.5337026084970782, 'gamma': 18.181720411194487, 'learning_rate': 0.19103547144687613, 'max_depth': 8, 'min_child_weight': 5, 'n_estimators': 22, 'reg_alpha': 3.2751653385716795, 'reg_lambda': 2.388915568063122, 'subsample': 0.9007720605272558}\n",
      "Mean accuracy: 0.9459770114942528\n",
      "Mean error: 5.824779026270237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9459770114942528, 5.824779026270237)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def HyperOptimize(X, y, multiplier = 1):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = (X.loc[X['year'] < 2022, :], X.loc[X['year'] == 2022, :], \n",
    "                                        y.loc[X['year'] < 2022], y.loc[X['year'] == 2022])\n",
    "\n",
    "    # Create fold structure so we can make a custom cross-validation for time-series\n",
    "    folds = [\n",
    "        (range(2002, 2006, 2), [2006, 2008]),\n",
    "        (range(2002, 2010, 2), [2010, 2012]),\n",
    "        (range(2002, 2014, 2), [2014, 2016]),\n",
    "        (range(2002, 2018, 2), [2018, 2020])\n",
    "    ]\n",
    "\n",
    "    cv = CustomTimeSeriesCV(folds)\n",
    "        \n",
    "    one_hot_fts = ['office_type', 'final_rating', 'open_seat']\n",
    "    std_fts = ['incumbent_margin', 'covi_num','special', 'prev_gb_margin', 'prev2_gb_margin',\n",
    "        'mean_specials_differential', 'pvi', 'previous_cci', 'current_cci',\n",
    "        'previous_gas', 'current_gas',  'previous_unemployment',\n",
    "        'current_unemployment', 'absenteeexcusereq', 'pollhours',\n",
    "        'avgpollhours', 'maxpollhours', 'minpollhours', 'regdeadlines',\n",
    "        'voteridlaws', 'novoterid', 'noallmailvote', 'noearlyvote',\n",
    "        'nofelonreg', 'nofelonsregafterincar', 'nonstrictid', 'nonstrictphoto',\n",
    "        'noonlineregistration', 'nopermanentabsentee', 'nopollplacereg', 'nopr',\n",
    "        'nosamedayreg', 'nostateholiday', 'pr16', 'pr17', 'pr175', 'pr60',\n",
    "        'pr90', 'strictid', 'strictphoto', 'house_chamber_margin',\n",
    "        'senate_chamber_margin', 'change_cci', 'change_unemployment']\n",
    "        \n",
    "    preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(), one_hot_fts), \n",
    "    ('num', 'passthrough', std_fts)])\n",
    "\n",
    "    n_states = 2\n",
    "    accuracies = np.zeros(n_states)\n",
    "    errors = np.zeros(n_states)\n",
    "    \n",
    "    #Goes through each random state, and finds the best parameters and error for each one\n",
    "    for random_state in range(0, n_states):\n",
    "        def objective(params):\n",
    "            clf = xgboost.XGBRegressor(**params, random_state = random_state, n_jobs=-1)\n",
    "            pipe = make_pipeline(preprocessor, clf)\n",
    "\n",
    "            cv_scores = []\n",
    "            for train_idx, test_idx in cv.split(X_train):\n",
    "                pipe.fit(X_train.iloc[train_idx], y_train.iloc[train_idx])\n",
    "                predictions = pipe.predict(X_train.iloc[test_idx])\n",
    "                max_year = X_train.iloc[test_idx]['year'].max()\n",
    "                score = (max_year - 2002) / 16 * incorrect_penalizer(y_train.iloc[test_idx], predictions, multiplier=multiplier)\n",
    "                cv_scores.append(score)\n",
    "\n",
    "            print(cv_scores)\n",
    "            avg_mae = np.mean(cv_scores)\n",
    "            return {'loss': avg_mae, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "        space = {\n",
    "            'n_estimators': hp.randint('n_estimators', 10, 251),\n",
    "            'max_depth': hp.randint('max_depth', 2, 10),\n",
    "            'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.2)),\n",
    "            'subsample': hp.uniform('subsample', 0.3, 1),\n",
    "            'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1),\n",
    "            'min_child_weight': hp.randint('min_child_weight', 5, 10),\n",
    "            'gamma': hp.loguniform('gamma', np.log(5), np.log(20)),\n",
    "            'reg_alpha': hp.loguniform('reg_alpha', np.log(0.1), np.log(10)),\n",
    "            'reg_lambda': hp.loguniform('reg_lambda', np.log(0.1), np.log(10))\n",
    "        }\n",
    "\n",
    "\n",
    "        trials = Trials()\n",
    "        best_params = fmin(fn=objective,\n",
    "                        space=space,\n",
    "                        algo=tpe.suggest,\n",
    "                        trials=trials, \n",
    "                        early_stop_fn=no_progress_loss(iteration_stop_count=5), \n",
    "                        rstate=np.random.Generator(np.random.PCG64(random_state)))\n",
    "\n",
    "        print(\"Best parameters:\", best_params)\n",
    "        acc_xgb = xgboost.XGBRegressor(**best_params, n_jobs=-1)\n",
    "        pipe = make_pipeline(preprocessor, acc_xgb)\n",
    "        pipe.fit(X_train, y_train)\n",
    "        predictions = pipe.predict(X_test)\n",
    "        errors[random_state] = median_absolute_error(y_test, predictions)\n",
    "        accuracies[random_state] = np.mean(np.sign(y_test) == np.sign(predictions))\n",
    "\n",
    "    print(\"Mean accuracy:\", np.mean(accuracies))\n",
    "    print(\"Mean error:\", np.mean(errors))\n",
    "    return np.mean(accuracies), np.mean(errors)\n",
    "\n",
    "HyperOptimize(X, y, multiplier = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.171101380878282, 6.5865965443073, 8.275412054249987, 7.338798270486502]\n",
      "[9.010894916669361, 14.530545147834024, 20.742816081308618, 24.381620082214035]                                  \n",
      "[7.101596134192656, 11.941281858551243, 16.37303221004745, 18.594731796331853]                                   \n",
      "[4.721484304061629, 7.974138844969804, 10.426546883203311, 10.314629735929726]                                   \n",
      "  0%|          | 4/9223372036854775807 [00:07<5032655455056182:02:40,  1.96s/trial, best loss: 6.592977062480518]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m median_errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m multiplier \u001b[38;5;129;01min\u001b[39;00m multipliers:\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mHyperOptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultiplier\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 70\u001b[0m, in \u001b[0;36mHyperOptimize\u001b[0;34m(X, y, multiplier)\u001b[0m\n\u001b[1;32m     56\u001b[0m space \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m251\u001b[39m),\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m: hp\u001b[38;5;241m.\u001b[39mloguniform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m0.1\u001b[39m), np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m     66\u001b[0m }\n\u001b[1;32m     69\u001b[0m trials \u001b[38;5;241m=\u001b[39m Trials()\n\u001b[0;32m---> 70\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_progress_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration_stop_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPCG64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[1;32m     78\u001b[0m acc_xgb \u001b[38;5;241m=\u001b[39m xgboost\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/hyperopt/fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/hyperopt/base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[0;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[31], line 45\u001b[0m, in \u001b[0;36mHyperOptimize.<locals>.objective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     43\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, test_idx \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X_train):\n\u001b[0;32m---> 45\u001b[0m     \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_train\u001b[38;5;241m.\u001b[39miloc[test_idx])\n\u001b[1;32m     47\u001b[0m     max_year \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39miloc[test_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/sklearn/pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    419\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 420\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/xgboost/training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data1030/lib/python3.11/site-packages/xgboost/core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "multipliers = np.linspace(1, 5, 10)\n",
    "accuracy_scores = []\n",
    "median_errors = []\n",
    "\n",
    "for multiplier in multipliers:\n",
    "    HyperOptimize(X, y, multiplier)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
