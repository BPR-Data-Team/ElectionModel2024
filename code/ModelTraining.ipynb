{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import lightgbm as lgb\n",
    "import xgboost\n",
    "import re\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.metrics import  accuracy_score, median_absolute_error, make_scorer\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "class CustomTimeSeriesCV(BaseCrossValidator):\n",
    "    \"\"\"Creates an iterator that contains the indices from each dataset based on the years given\"\"\"\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for train_years, test_years in self.years:\n",
    "            train_indices = np.where(X['year'].isin(train_years))[0]\n",
    "            test_indices = np.where(X['year'].isin(test_years))[0]\n",
    "            yield train_indices, test_indices\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return len(self.years) \n",
    "    \n",
    "def penalize_wrong(y, y_pred, penalty):\n",
    "    \"Penalizes wrong guesses more, determined by the value of k\"\n",
    "    return np.mean(abs((y_pred - y))*(1+penalty*(np.sign(y_pred)\n",
    "                                               != np.sign(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "#Categorical features that need to be one-hot encoded    \n",
    "one_hot_fts = ['office_type']\n",
    "\n",
    "#Rating is the only ordinal feature\n",
    "ordinal_fts = ['final_rating']\n",
    "ordinal_fts_ranking = ['Safe R', 'Likely R', 'Leans R', 'Toss-up', 'Leans D', 'Likely D', 'Safe D']\n",
    "\n",
    "#Cont features that should be pass-throughed (and later scaled)\n",
    "cont_fts = ['open_seat','incumbent_differential', 'absenteeexcusereq', 'special', 'isMidterm',\n",
    "       'pollhours', 'avgpollhours', 'minpollhours', 'regdeadlines',\n",
    "       'voteridlaws', 'novoterid', 'noallmailvote', 'noearlyvote',\n",
    "       'nofelonreg', 'nofelonsregafterincar', 'nonstrictid', 'nonstrictphoto',\n",
    "       'nopollplacereg', 'nopr', 'nosamedayreg', 'nostateholiday', 'pr16',\n",
    "       'pr17', 'pr175', 'pr60', 'pr90', 'strictid', 'strictphoto', 'covi_num',\n",
    "       'prev_gen_margin', 'prev_dem_gen_tp', 'weighted_genpoll', 'unweighted_genpoll',\n",
    "       'mean_specials_differential', 'house_chamber_margin',\n",
    "       'senate_chamber_margin', 'previous_cci', 'current_cci', 'change_cci',\n",
    "       'previous_gas', 'current_gas', 'change_gas', 'previous_unemployment',\n",
    "       'current_unemployment', 'change_unemployment', 'receipts_DEM',\n",
    "       'receipts_REP', 'disbursements_DEM', 'disbursements_REP',\n",
    "       'unconvinced_pct', 'phone_unweighted', 'online_unweighted', 'num_polls',\n",
    "       'unweighted_estimate', 'unweighted_ci_lower', 'unweighted_ci_upper',\n",
    "       'weighted_estimate', 'weighted_ci_lower', 'weighted_ci_upper',\n",
    "       'white_pct', 'black_pct', 'asian_pct', 'hispanic_pct', 'median_income',\n",
    "       'impoverished_pct', 'median_age', 'renting_pct', 'inflation',\n",
    "       'isMidterm', 'receipts_ratio', 'disbursements_ratio', 'total_receipts',\n",
    "       'total_disbursements', 'genballot_predicted_margin',\n",
    "       'specials_predicted_margin', 'receipts_genballot_interaction', 'poll_fundamental_agree',\n",
    "       'disbursements_genballot_interaction', 'gas_democrat_interaction', 'cci_democrat_interaction', 'genballot_predicted_lower',\n",
    "       'genballot_predicted_upper', 'democrat_in_presidency', 'similar_poll_differential', 'combined_prediction']\n",
    "\n",
    "def optima_model(model, param_dict, X, y, penalizing_factor = 10):\n",
    "    \"\"\"Performs hyperparameter optimization for a a given bootstrapped X \n",
    "    ## Parameters:\n",
    "    model: sklearnable model. We use LGBMRegressor \n",
    "    param_dict: dictionary of hyperparameters to optimize\n",
    "    X: DataFrame with features\n",
    "    y: Series with target variable\"\"\"\n",
    "    \n",
    "    X_other, y_other = X.loc[X['year'] <= 2022, :], y.loc[X['year'] <= 2022]\n",
    "    X_train, X_test, y_train, y_test = (X.loc[X['year'] < 2022, :], X.loc[X['year'] == 2022, :], \n",
    "                                        y.loc[X['year'] < 2022], y.loc[X['year'] == 2022])\n",
    "    \n",
    "    # Create fold structure so we can make a custom cross-validation for time-series\n",
    "    folds = [\n",
    "        (range(2002, 2006, 2), [2006, 2008]),\n",
    "        (range(2002, 2010, 2), [2010, 2012]),\n",
    "        (range(2002, 2014, 2), [2014, 2016]),\n",
    "        (range(2002, 2018, 2), [2020])\n",
    "    ]\n",
    "\n",
    "    cv = CustomTimeSeriesCV(folds)\n",
    "        \n",
    "    #Preprocessing data: no need to scale data, because we use tree-based models which are monotonic-scale-invariant\n",
    "    #Because we don't need to scale data, we don't have to include the column transformer in the final saved model\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts),\n",
    "        ('ord', OrdinalEncoder(categories = [ordinal_fts_ranking], handle_unknown='use_encoded_value', \n",
    "                               unknown_value=np.nan), ordinal_fts),\n",
    "        ('num', 'passthrough', cont_fts)])\n",
    "    \n",
    "    def objective(params):\n",
    "        print(\"Params: \", params)\n",
    "        \"Function that takes in hyperparameters and returns loss, that Hyperopt will minimize.\"        \n",
    "        testing_loss = [] \n",
    "        accuracies = []\n",
    "        for train_idx, test_idx in cv.split(X_train):\n",
    "            reg = model(**params)\n",
    "            pipe = Pipeline(steps = [\n",
    "                ('preprocessing', preprocessor), \n",
    "                ('model', reg)])\n",
    "            \n",
    "            \n",
    "            #Necessary steps to utilize early-stopping on LGBM\n",
    "            pipe.named_steps['preprocessing'].fit(X_train.iloc[train_idx])\n",
    "            transformed_val = pipe.named_steps['preprocessing'].transform(X_train.iloc[test_idx])\n",
    "            penalize_scorer = make_scorer(penalize_wrong, greaterisbetter=False, penalizing_factor = penalizing_factor)\n",
    "\n",
    "            \n",
    "            \"\"\"Goes through each fold and calculates loss.\n",
    "            Note: We use median absolute error because it is more robust to outliers than mean absolute error.\n",
    "            We also expect earlier folds to have higher error, since they have less data to train on.\"\"\"\n",
    "            early_stopping = lgb.early_stopping(10, verbose = False)\n",
    "            \n",
    "            pipe.fit(X_train.iloc[train_idx], y_train.iloc[train_idx],\n",
    "                     model__eval_set = [(transformed_val, y_train.iloc[test_idx])], model__eval_metric = 'mae', \n",
    "                     model__callbacks = [early_stopping])            \n",
    "            \n",
    "            \n",
    "            predictions = pipe.predict(X_train.iloc[test_idx])\n",
    "            testing_loss.append(penalize_wrong(y_train.iloc[test_idx], predictions, penalizing_factor))\n",
    "            accuracies.append(accuracy_score(np.sign(y_train.iloc[test_idx]), np.sign(predictions)))\n",
    "            \n",
    "        mean_test_loss = np.mean(testing_loss)\n",
    "        print(f\"Validation loss: {testing_loss}, mean: {mean_test_loss}\")\n",
    "        print(f\"Validation accuracy: {accuracies}, mean: {np.mean(accuracies)}\")\n",
    "        return {'loss': mean_test_loss, 'status': STATUS_OK}\n",
    "\n",
    "    start_time = time.time()\n",
    "    max_time = 20 #about two minutes per run-through\n",
    "    def stop(trial, elapsed_time=0):\n",
    "        return elapsed_time > max_time, [time.time() - start_time] \n",
    "    \n",
    "    \"Hyperopt uses the TPE algorithm to optimize hyperparameters. We use the no_progress_loss function to stop early if we don't see progress.\"\n",
    "    best_params = fmin(fn=objective,\n",
    "                    space=param_dict,\n",
    "                    algo=tpe.suggest,\n",
    "                    trials=Trials(),\n",
    "                    early_stop_fn = stop)\n",
    "                    \n",
    "                    \n",
    "    print(\"Best parameters:\", best_params)\n",
    "    best_model = model(**best_params)\n",
    "    pipe = Pipeline(steps = [\n",
    "        ('preprocessing', preprocessor), \n",
    "        ('model', best_model)])\n",
    "    \n",
    "    #Returns a fitted ML algortithm with those hyperparameters\n",
    "    pipe.fit(X_other, y_other) \n",
    "    #Returns the final model   \n",
    "    return pipe.named_steps['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params:                                                                \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.5946499761983302, 'learning_rate': 0.12677193009772983, 'max_depth': 6, 'min_child_samples': 149, 'min_data_in_bin': 9, 'min_data_in_leaf': 8, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 36, 'reg_alpha': 0.5728368538425305, 'reg_lambda': 2.0121595466177173, 'subsample': 0.7129112953810639, 'subsample_for_bin': 23745, 'verbose': -1}\n",
      "Validation loss: [16.75447591389995, 12.759005065600467, 12.042767254375418, 12.906633239397518], mean: 13.615720368318339\n",
      "Validation accuracy: [0.9420131291028446, 0.950836820083682, 0.9537366548042705, 0.936734693877551], mean: 0.945830324467087\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.5857143265040059, 'learning_rate': 0.04589937471022834, 'max_depth': 3, 'min_child_samples': 135, 'min_data_in_bin': 3, 'min_data_in_leaf': 6, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 39, 'reg_alpha': 1.399354956400023, 'reg_lambda': 2.0664211298978192, 'subsample': 0.79263390401229, 'subsample_for_bin': 198254, 'verbose': -1}\n",
      "Validation loss: [17.86359514742515, 12.024940550534145, 12.224156674142629, 13.875228940099486], mean: 13.996980328050352\n",
      "Validation accuracy: [0.9321663019693655, 0.9581589958158996, 0.9537366548042705, 0.9285714285714286], mean: 0.943158345290241\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.5449030363641478, 'learning_rate': 0.005513163585793035, 'max_depth': 2, 'min_child_samples': 69, 'min_data_in_bin': 4, 'min_data_in_leaf': 2, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 44, 'reg_alpha': 0.4627832781330757, 'reg_lambda': 1.8082884222222042, 'subsample': 0.7216536288899009, 'subsample_for_bin': 145721, 'verbose': -1}\n",
      "Validation loss: [39.84898117203718, 26.96139503330153, 26.201670005041024, 29.602636352647103], mean: 30.653670640756708\n",
      "Validation accuracy: [0.888402625820569, 0.9194560669456067, 0.9205219454329775, 0.8979591836734694], mean: 0.9065849554681557\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.5301258582878116, 'learning_rate': 0.004895124191107329, 'max_depth': 7, 'min_child_samples': 59, 'min_data_in_bin': 5, 'min_data_in_leaf': 2, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 58, 'reg_alpha': 1.9828070904848034, 'reg_lambda': 2.086604511905067, 'subsample': 0.5954696640041851, 'subsample_for_bin': 47011, 'verbose': -1}\n",
      "Validation loss: [24.45886413988992, 21.86930749216652, 24.84231149258624, 25.6933118736144], mean: 24.215948749564273\n",
      "Validation accuracy: [0.9332603938730853, 0.9571129707112971, 0.9205219454329775, 0.889795918367347], mean: 0.9251728070961768\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.43500067569505496, 'learning_rate': 0.0029333159519311434, 'max_depth': 13, 'min_child_samples': 137, 'min_data_in_bin': 3, 'min_data_in_leaf': 4, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 40, 'reg_alpha': 0.6813092422106048, 'reg_lambda': 1.7647727212770001, 'subsample': 0.5153584048904237, 'subsample_for_bin': 146258, 'verbose': -1}\n",
      "Validation loss: [28.093942951366383, 28.250997405028514, 29.702981017406195, 30.521699050572455], mean: 29.14240510609339\n",
      "Validation accuracy: [0.9321663019693655, 0.9309623430962343, 0.9039145907473309, 0.8816326530612245], mean: 0.9121689722185388\n",
      "Params:                                                                                                            \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.7491330625229973, 'learning_rate': 0.015691737793830495, 'max_depth': 2, 'min_child_samples': 120, 'min_data_in_bin': 3, 'min_data_in_leaf': 1, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 26, 'reg_alpha': 0.0043223493747432395, 'reg_lambda': 0.26220391310049607, 'subsample': 0.65720479596399, 'subsample_for_bin': 137937, 'verbose': -1}\n",
      "Validation loss: [32.22382833956835, 17.781099862813303, 18.130499782947844, 21.956155529961368], mean: 22.522895878822716\n",
      "Validation accuracy: [0.8927789934354485, 0.9414225941422594, 0.9395017793594306, 0.9020408163265307], mean: 0.9189360458159174\n",
      "Params:                                                                                                            \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.6689585713374409, 'learning_rate': 0.004781076162021382, 'max_depth': 9, 'min_child_samples': 87, 'min_data_in_bin': 8, 'min_data_in_leaf': 3, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 44, 'reg_alpha': 2.2075309547935182, 'reg_lambda': 0.12094321929144836, 'subsample': 0.6235217205383926, 'subsample_for_bin': 71747, 'verbose': -1}\n",
      "Validation loss: [25.327281493946586, 22.209159813398863, 25.863398384311978, 25.414504235379294], mean: 24.703585981759183\n",
      "Validation accuracy: [0.9321663019693655, 0.9539748953974896, 0.9169632265717675, 0.8959183673469387], mean: 0.9247556978213903\n",
      "Params:                                                                                                            \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.7658880200114278, 'learning_rate': 0.004956518783105562, 'max_depth': 5, 'min_child_samples': 37, 'min_data_in_bin': 8, 'min_data_in_leaf': 3, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 47, 'reg_alpha': 1.7180254856447612, 'reg_lambda': 1.430776106157973, 'subsample': 0.558148781249466, 'subsample_for_bin': 140370, 'verbose': -1}\n",
      "Validation loss: [26.38572573967241, 22.157624690415005, 25.733526831763434, 25.31271209872866], mean: 24.897397340144877\n",
      "Validation accuracy: [0.9234135667396062, 0.9560669456066946, 0.9169632265717675, 0.9], mean: 0.924110934729517    \n",
      "  0%|          | 8/9223372036854775807 [00:27<8703640816051377:29:36,  3.40s/trial, best loss: 13.615720368318339] \n",
      "Best parameters: {'colsample_bytree': 0.5946499761983302, 'learning_rate': 0.12677193009772983, 'max_depth': 6, 'min_child_samples': 149, 'min_data_in_bin': 9, 'min_data_in_leaf': 8, 'num_leaves': 36, 'reg_alpha': 0.5728368538425305, 'reg_lambda': 2.0121595466177173, 'subsample': 0.7129112953810639, 'subsample_for_bin': 23745}\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=149 will be ignored. Current value: min_data_in_leaf=8\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=149 will be ignored. Current value: min_data_in_leaf=8\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8427\n",
      "[LightGBM] [Info] Number of data points in the train set: 4979, number of used features: 90\n",
      "[LightGBM] [Info] Start training from score 1.531637\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Params:                                                                \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.6222487994917242, 'learning_rate': 0.011604473222713425, 'max_depth': 12, 'min_child_samples': 53, 'min_data_in_bin': 2, 'min_data_in_leaf': 6, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 56, 'reg_alpha': 0.4101675620996806, 'reg_lambda': 0.6724970439339395, 'subsample': 0.7221468222696823, 'subsample_for_bin': 136225, 'verbose': -1}\n",
      "Validation loss: [20.392427386767466, 15.146040722290879, 15.499813635913306, 14.35428867244203], mean: 16.34814260435342\n",
      "Validation accuracy: [0.9277899343544858, 0.9497907949790795, 0.9359430604982206, 0.9469387755102041], mean: 0.9401156413354974\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.7379570516402421, 'learning_rate': 0.06728678165235359, 'max_depth': 2, 'min_child_samples': 119, 'min_data_in_bin': 2, 'min_data_in_leaf': 1, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 28, 'reg_alpha': 1.890192759717473, 'reg_lambda': 2.183529026352941, 'subsample': 0.6705007064243396, 'subsample_for_bin': 67395, 'verbose': -1}\n",
      "Validation loss: [25.69842730412861, 14.796821059361688, 14.373085161639608, 11.521834791508349], mean: 16.597542079159563\n",
      "Validation accuracy: [0.8971553610503282, 0.9393305439330544, 0.9395017793594306, 0.9510204081632653], mean: 0.9317520231265196\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.5221726679214933, 'learning_rate': 0.002780672212866325, 'max_depth': 13, 'min_child_samples': 66, 'min_data_in_bin': 4, 'min_data_in_leaf': 7, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 60, 'reg_alpha': 1.6365248470487461, 'reg_lambda': 2.374893796089305, 'subsample': 0.5123879227707666, 'subsample_for_bin': 81584, 'verbose': -1}\n",
      "Validation loss: [30.10504767712073, 29.78681993824858, 30.36324956588385, 27.08194127312978], mean: 29.334264613595735\n",
      "Validation accuracy: [0.912472647702407, 0.899581589958159, 0.896797153024911, 0.9061224489795918], mean: 0.9037434599162673\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.7405226826583005, 'learning_rate': 0.0063272527517176606, 'max_depth': 5, 'min_child_samples': 61, 'min_data_in_bin': 3, 'min_data_in_leaf': 5, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 44, 'reg_alpha': 1.1859989731622922, 'reg_lambda': 2.0623449751968, 'subsample': 0.5609703015367526, 'subsample_for_bin': 31035, 'verbose': -1}\n",
      "Validation loss: [26.00390842168465, 21.48572689870602, 21.048671098465917, 17.595569387506686], mean: 21.53346895159082\n",
      "Validation accuracy: [0.9135667396061269, 0.944560669456067, 0.9240806642941874, 0.9510204081632653], mean: 0.9333071203799116\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.48370997299984414, 'learning_rate': 0.2605956186424049, 'max_depth': 12, 'min_child_samples': 117, 'min_data_in_bin': 7, 'min_data_in_leaf': 6, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 60, 'reg_alpha': 0.953784724273774, 'reg_lambda': 0.36474827914647595, 'subsample': 0.7050275475730847, 'subsample_for_bin': 194816, 'verbose': -1}\n",
      "Validation loss: [19.022572006255984, 14.300836001077872, 11.46861122118236, 13.161555125556813], mean: 14.488393588518257\n",
      "Validation accuracy: [0.912472647702407, 0.944560669456067, 0.9525504151838672, 0.9306122448979591], mean: 0.9350489943100752\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.7895320934892713, 'learning_rate': 0.3472444834962701, 'max_depth': 12, 'min_child_samples': 92, 'min_data_in_bin': 5, 'min_data_in_leaf': 5, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 29, 'reg_alpha': 1.026817651239508, 'reg_lambda': 2.0324669059292892, 'subsample': 0.6044231264958034, 'subsample_for_bin': 124789, 'verbose': -1}\n",
      "Validation loss: [21.407142593381224, 13.290618956895042, 13.18980393621636, 12.230671116040524], mean: 15.02955915063329\n",
      "Validation accuracy: [0.9168490153172867, 0.9393305439330544, 0.9406880189798339, 0.9428571428571428], mean: 0.9349311802718294\n",
      "Params:                                                                                                           \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.5277883910054353, 'learning_rate': 0.002794230521578861, 'max_depth': 13, 'min_child_samples': 73, 'min_data_in_bin': 1, 'min_data_in_leaf': 8, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 58, 'reg_alpha': 0.07412511368796754, 'reg_lambda': 0.5843509736984853, 'subsample': 0.7466528547350622, 'subsample_for_bin': 166641, 'verbose': -1}\n",
      "Validation loss: [29.92298300301629, 28.593196534452716, 30.11383313251212, 26.61036096280207], mean: 28.810093408195797\n",
      "Validation accuracy: [0.912472647702407, 0.9058577405857741, 0.8991696322657177, 0.9102040816326531], mean: 0.906926025546638\n",
      "Params:                                                                                                            \n",
      "{'boosting_type': 'gbdt', 'colsample_bytree': 0.665321056663468, 'learning_rate': 0.027340567650006184, 'max_depth': 3, 'min_child_samples': 116, 'min_data_in_bin': 4, 'min_data_in_leaf': 8, 'monotone_constraints': (0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0), 'num_leaves': 27, 'reg_alpha': 1.641708050420462, 'reg_lambda': 0.4285833957990057, 'subsample': 0.5703348388682461, 'subsample_for_bin': 140482, 'verbose': -1}\n",
      "Validation loss: [24.82526515090875, 14.707579884124723, 11.955091603567618, 10.53347323068352], mean: 15.505352467321153\n",
      "Validation accuracy: [0.8927789934354485, 0.9382845188284519, 0.9442467378410438, 0.9551020408163265], mean: 0.9326030727303177\n",
      "  0%|          | 8/9223372036854775807 [00:27<8780801716954653:34:56,  3.43s/trial, best loss: 14.488393588518257] \n",
      "Best parameters: {'colsample_bytree': 0.48370997299984414, 'learning_rate': 0.2605956186424049, 'max_depth': 12, 'min_child_samples': 117, 'min_data_in_bin': 7, 'min_data_in_leaf': 6, 'num_leaves': 60, 'reg_alpha': 0.953784724273774, 'reg_lambda': 0.36474827914647595, 'subsample': 0.7050275475730847, 'subsample_for_bin': 194816}\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=117 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=6, min_child_samples=117 will be ignored. Current value: min_data_in_leaf=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 8840\n",
      "[LightGBM] [Info] Number of data points in the train set: 4979, number of used features: 90\n",
      "[LightGBM] [Info] Start training from score 1.233850\n"
     ]
    }
   ],
   "source": [
    "#Bootstraps X and y, and then runs the optimization function\n",
    "def bootstrap(group, n=None):\n",
    "    if n is None:\n",
    "        n = len(group)\n",
    "    return group.sample(n, replace=True)\n",
    "\n",
    "data = pd.read_csv(\"../cleaned_data/Engineered Dataset.csv\")\n",
    "data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts),\n",
    "        ('ord', OrdinalEncoder(categories = [ordinal_fts_ranking], handle_unknown='use_encoded_value', \n",
    "                               unknown_value=np.nan), ordinal_fts),\n",
    "        ('num', 'passthrough', cont_fts)])\n",
    "\n",
    "names_for_monotonicity = preprocessor.fit(data.drop(columns=['margin'])).get_feature_names_out()\n",
    "before_processing_monotonic_columns = ['incumbent_differential', 'pvi', 'receipts_ratio', 'disbursements_ratio', \n",
    "                                       'genballot_predicted_margin', 'specials_predicted_margin', 'unweighted_estimate', 'unweighted_ci_lower',\n",
    "                                       'unweighted_ci_upper','weighted_estimate', 'weighted_ci_lower', 'weighted_ci_upper',\n",
    "                                       'phone_unweighted', 'online_unweighted', 'receipts_genballot_interaction',\n",
    "                                       'disbursements_genballot_interaction', 'poll_fundamental_average']\n",
    "\n",
    "monotonic_columns = ['num__' + name for name in before_processing_monotonic_columns]\n",
    "monotone_constraints = [1 if name in monotonic_columns else 0 for name in names_for_monotonicity]\n",
    "\n",
    "# Define the search space for Hyperopt\n",
    "param_dist_lgbm = {\n",
    "    'boosting_type': 'gbdt',  # Removed 'goss' to simplify\n",
    "    'num_leaves': hp.randint('num_leaves', 20, 70),  # Reduced the upper limit, \n",
    "    'learning_rate': hp.loguniform('learning_rate', -6, -1),  # Equivalent to about 0.0001 to 0.01\n",
    "    'subsample_for_bin': hp.randint('subsample_for_bin', 20000, 200000),  # Narrowed the range\n",
    "    'min_data_in_bin': hp.randint('min_data_in_bin', 1, 10), \n",
    "    'min_data_in_leaf': hp.randint('min_data_in_leaf', 1, 10),  # Reduced the upper limit\n",
    "    'min_child_samples': hp.randint('min_child_samples', 20, 150),  # Increased the range for more regularization\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 2.5),  # Increased upper limit for L1 regularization\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 2.5),  # Increased upper limit for L2 regularization\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.8),  # Reduced the upper limit\n",
    "    'subsample': hp.uniform('subsample', 0.5, 0.8),  # Reduced the upper limit for more randomness\n",
    "    'max_depth': hp.randint('max_depth', 2, 15),  # Added max_depth for additional control\n",
    "    \"verbose\": -1,  # Keep verbose to -1 to reduce log clutter, \n",
    "    'monotone_constraints': monotone_constraints\n",
    "}\n",
    "\n",
    "\n",
    "#optima_model(lgb.LGBMRegressor, param_dist_lgbm, data.drop(columns=['margin']), data['margin'])\n",
    "\n",
    "num_trials = 2\n",
    "for idx in range(num_trials):\n",
    "    bootstrapped_data = data.groupby(['year', 'office_type']).apply(bootstrap).reset_index(drop=True)\n",
    "    \n",
    "    bootstrapped_X = bootstrapped_data.drop(columns=['margin'])\n",
    "    bootstrapped_y = bootstrapped_data['margin']\n",
    "    \n",
    "    trained_lgbm = optima_model(lgb.LGBMRegressor, param_dist_lgbm, bootstrapped_X, bootstrapped_y)\n",
    "    file_path = f\"../models/Model_{idx}.pkl\"\n",
    "\n",
    "    # Open a file to write in binary mode????        \n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(trained_lgbm, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
