{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost\n",
    "from scipy.stats import loguniform, randint, t, uniform\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "class CustomTimeSeriesCV(BaseCrossValidator):\n",
    "    \"\"\"Creates an iterator that contains the indices from each dataset based on the years given\"\"\"\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for train_years, test_years in self.years:\n",
    "            train_indices = np.where(X['cycle'].isin(train_years))[0]\n",
    "            test_indices = np.where(X['cycle'].isin(test_years))[0]\n",
    "            yield train_indices, test_indices\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return len(self.years) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "polls_for_rating = pd.read_csv('../../data/raw_polls.csv')\n",
    "days_to_rate = 21\n",
    "\n",
    "office_type_dict = {\n",
    "    \"Pres-G\": \"President\",\n",
    "    \"Sen-G\": \"Senate\",\n",
    "    \"Gov-G\": \"Governor\",\n",
    "    \"House-G\": \"House\"    \n",
    "}\n",
    "\n",
    "#This filters out rows we do not want\n",
    "polls_for_rating = polls_for_rating.query(\"time_to_election <= @days_to_rate & not @pd.isna(methodology)\")\n",
    "polls_for_rating = polls_for_rating[(polls_for_rating['cand1_party'] == \"DEM\") & (polls_for_rating['cand2_party'] == \"REP\") & (polls_for_rating['location'] != \"US\")]\n",
    "polls_for_rating = polls_for_rating[polls_for_rating['type_simple'].isin([\"Pres-G\", \"Sen-G\", \"Gov-G\", \"House-G\"])]\n",
    "\n",
    "#Adding important columns, X and Y\n",
    "polls_for_rating['office_type'] = polls_for_rating['type_simple'].map(office_type_dict)\n",
    "polls_for_rating['bias'] = polls_for_rating['margin_poll'] - polls_for_rating['margin_actual']\n",
    "polls_for_rating['error'] = np.abs(polls_for_rating['bias']) \n",
    "polls_for_rating['greater than 20'] = abs(polls_for_rating['margin_poll']) > 20\n",
    "\n",
    "polls_for_rating = polls_for_rating[['cycle', 'office_type', 'pollster_rating_id', 'aapor_roper', 'methodology', \n",
    "                                     'partisan', 'samplesize', 'greater than 20', 'bias', 'error']]\n",
    "\n",
    "unique_methods = set()\n",
    "for methods in polls_for_rating['methodology']:\n",
    "    unique_methods.update(methods.split('/'))\n",
    "\n",
    "for method in unique_methods:\n",
    "    polls_for_rating[method] = polls_for_rating['methodology'].apply(lambda x: 1 if method in x.split('/') else 0)\n",
    "\n",
    "polls_for_rating = polls_for_rating.drop(columns=['methodology'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pollster_rating_model(poll_df, before_year, model, param_dist):\n",
    "    \"\"\"This creates and saves two models for polls before a given year: one to predict error, one to predict bias.\n",
    "    It is very important that we do NOT include polls from the before year in the data\"\"\"\n",
    "    poll_df = poll_df[poll_df['cycle'] < before_year]\n",
    "    X = poll_df.drop(columns=['bias', 'error'])\n",
    "    error = poll_df['error']\n",
    "    bias = poll_df['bias']\n",
    "    \n",
    "    dummy_creator = OneHotEncoder(sparse_output=False, handle_unknown='ignore', min_frequency=20) #Only choosing pollster with 20 or more previous polls\n",
    "\n",
    "    preprocessor = ColumnTransformer([('cat', dummy_creator, ['pollster_rating_id', 'partisan', 'office_type'])], remainder='passthrough')\n",
    "\n",
    "    min_year = poll_df['cycle'].min()\n",
    "    \n",
    "    folds = [(range(min_year, year), [year]) for year in range(min_year + 2, before_year, 2)] #Dynamically creating folds based on the before year\n",
    "    cv = CustomTimeSeriesCV(folds)\n",
    "\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "\n",
    "    param_dist = {\n",
    "    'regressor__' + key: value for key, value in param_dist.items()\n",
    "    }\n",
    "    \n",
    "    grid = RandomizedSearchCV(model, param_dist, n_iter=50, cv=cv, scoring='neg_mean_squared_error', random_state=42, n_jobs=-1, verbose =1)\n",
    "    grid.fit(X, error)\n",
    "    \n",
    "    print(f'Best error model for {before_year}: {model.__class__.__name__} with MAE of {grid.best_score_}')\n",
    "    \n",
    "    file_path_error = f'../../models/Polls_{before_year}_error.pkl'\n",
    "    with open(file_path_error, 'wb') as file:\n",
    "        pkl.dump(grid, file)\n",
    "\n",
    "    grid.fit(X, bias)\n",
    "    file_path_bias = f'../../models/Polls_{before_year}_bias.pkl'\n",
    "    with open(file_path_bias, 'wb') as file:\n",
    "        pkl.dump(grid, file)\n",
    "        \n",
    "    print(f'Best bias model for {before_year}: {model.__class__.__name__} with MAE of {grid.best_score_}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n",
      "Best error model for 2002: Pipeline with MAE of -20.722353635181666\n",
      "Fitting 1 folds for each of 50 candidates, totalling 50 fits\n",
      "Best bias model for 2002: Pipeline with MAE of -43.857672232069504\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Best error model for 2004: Pipeline with MAE of -18.65201850806786\n",
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n",
      "Best bias model for 2004: Pipeline with MAE of -45.481264609014815\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best error model for 2006: Pipeline with MAE of -16.75717514518871\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "Best bias model for 2006: Pipeline with MAE of -40.30218726308426\n",
      "Fitting 4 folds for each of 50 candidates, totalling 200 fits\n",
      "Best error model for 2008: Pipeline with MAE of -16.66048200617618\n",
      "Fitting 4 folds for each of 50 candidates, totalling 200 fits\n",
      "Best bias model for 2008: Pipeline with MAE of -41.78580601963078\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best error model for 2010: Pipeline with MAE of -15.918640187864677\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Best bias model for 2010: Pipeline with MAE of -39.59616071258863\n",
      "Fitting 6 folds for each of 50 candidates, totalling 300 fits\n",
      "Best error model for 2012: Pipeline with MAE of -17.25409879321927\n",
      "Fitting 6 folds for each of 50 candidates, totalling 300 fits\n",
      "Best bias model for 2012: Pipeline with MAE of -41.62214918704782\n",
      "Fitting 7 folds for each of 50 candidates, totalling 350 fits\n",
      "Best error model for 2014: Pipeline with MAE of -16.642581037449578\n",
      "Fitting 7 folds for each of 50 candidates, totalling 350 fits\n",
      "Best bias model for 2014: Pipeline with MAE of -39.81552124003843\n",
      "Fitting 8 folds for each of 50 candidates, totalling 400 fits\n",
      "Best error model for 2016: Pipeline with MAE of -17.15975256263007\n",
      "Fitting 8 folds for each of 50 candidates, totalling 400 fits\n",
      "Best bias model for 2016: Pipeline with MAE of -39.86907279500336\n",
      "Fitting 9 folds for each of 50 candidates, totalling 450 fits\n",
      "Best error model for 2018: Pipeline with MAE of -17.29192791697431\n",
      "Fitting 9 folds for each of 50 candidates, totalling 450 fits\n",
      "Best bias model for 2018: Pipeline with MAE of -39.46247931618311\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
      "Best error model for 2020: Pipeline with MAE of -17.028726388186804\n",
      "Fitting 10 folds for each of 50 candidates, totalling 500 fits\n",
      "Best bias model for 2020: Pipeline with MAE of -39.167776224708085\n",
      "Fitting 11 folds for each of 50 candidates, totalling 550 fits\n",
      "Best error model for 2022: Pipeline with MAE of -16.785033831640607\n",
      "Fitting 11 folds for each of 50 candidates, totalling 550 fits\n",
      "Best bias model for 2022: Pipeline with MAE of -39.04975594585948\n",
      "Fitting 12 folds for each of 50 candidates, totalling 600 fits\n",
      "Best error model for 2024: Pipeline with MAE of -16.6994352206253\n",
      "Fitting 12 folds for each of 50 candidates, totalling 600 fits\n",
      "Best bias model for 2024: Pipeline with MAE of -40.28447556841546\n"
     ]
    }
   ],
   "source": [
    "param_dist_xgb = {\n",
    "    'n_estimators': randint(50, 1000),  # Number of boosted trees to fit\n",
    "    'max_depth': randint(2, 15),  # Maximum tree depth for base learners\n",
    "    'learning_rate': loguniform(0.01, 0.3),  # Boosting learning rate\n",
    "    'gamma': loguniform(0.001, 5),  # Minimum loss reduction required to make a further partition\n",
    "    'min_child_weight': loguniform(0.1, 10),  # Minimum sum of instance weight (hessian) needed in a child\n",
    "    'subsample': uniform(0.5, 0.5),  # Subsample ratio of the training instances\n",
    "    'colsample_bytree': uniform(0.5, 0.5),  # Subsample ratio of columns when constructing each tree\n",
    "    'colsample_bylevel': uniform(0.5, 0.5),  # Subsample ratio of columns for each level\n",
    "    'colsample_bynode': uniform(0.5, 0.5),  # Subsample ratio of columns for each node (split)\n",
    "    'reg_alpha': loguniform(0.01, 100),  # L1 regularization term on weights\n",
    "    'reg_lambda': loguniform(0.01, 100),  # L2 regularization term on weights\n",
    "    'scale_pos_weight': loguniform(0.1, 10),  # Balancing of positive and negative weights\n",
    "    'max_delta_step': randint(0, 10),  # Maximum delta step we allow each tree's weight estimation to be\n",
    "}\n",
    "\n",
    "years = range(2002, 2026, 2)\n",
    "for year in years:\n",
    "    make_pollster_rating_model(polls_for_rating, year, xgboost.XGBRegressor(), param_dist_xgb)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
