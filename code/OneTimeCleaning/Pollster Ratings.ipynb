{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost\n",
    "from scipy.stats import loguniform, randint, t\n",
    "\n",
    "\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "methodologies = [\"Live Phone\", \"IVR\", \"Online\", \"Text\", \"Mail\", \"Probability Panel\", \"Text-to-Web\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to Calculate Pollster Rating\n",
    "This project works in the following way:\n",
    "1. For every year, look at all polls from previous years\n",
    "2. Create a prediction algorithm from all non-pollster values (sample size, methodology, partisan, samplesize, days_before_election) and use them to predict the error via XGBoost\n",
    "3. Then, get the best model's predictions for each value, and place it back into the original dataset\n",
    "4. Calculate how much better each pollster is than what we'd expect from that pollster, given the other data points we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_estimator(X, y):\n",
    "    \"\"\"Runs through XGBoost to get the expected error based on non-pollster values (partisan, samplesize, etc.). \n",
    "    Returns the estimator that predicts that error the best.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "    \n",
    "    one_hot_fts = ['office_type', 'partisan']\n",
    "    std_fts = ['sample_size', 'days_before_election'] + methodologies\n",
    "    preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(), one_hot_fts),\n",
    "    ('std', StandardScaler(), std_fts)])\n",
    "    \n",
    "    cv = KFold(n_splits = 4)\n",
    "    xgb = xgboost.XGBRegressor()\n",
    "    model_name = xgb.__class__.__name__\n",
    "    \n",
    "    param_dict = {\n",
    "        'n_estimators': randint(10, 200), \n",
    "        'max_depth': randint(2, 12), \n",
    "        'eta': loguniform(0.001, 1), \n",
    "        'reg_alpha': loguniform(0.01, 100), \n",
    "        'reg_lambda': loguniform(0.01, 100)\n",
    "    }\n",
    "    \n",
    "    param_dict = {f\"{model_name.lower()}__{key}\": value for key, value in param_dict.items()}\n",
    "    \n",
    "    pipe = make_pipeline(preprocessor, xgb)\n",
    "    \n",
    "    grid = RandomizedSearchCV(pipe, param_distributions=param_dict, n_iter = 75, scoring='neg_mean_squared_error', cv = cv, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    #Code only used if I want to debug and see how good the model is doing\n",
    "    test_score = mean_absolute_error(y_test, grid.predict(X_test))\n",
    "    print(f\"Test MAE is {test_score}\")\n",
    "    \n",
    "    return (grid.best_estimator_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS:\n",
    "years_to_rate = 10\n",
    "\n",
    "def conf_width(errors):\n",
    "        \"\"\"Calculates the length of the one-sided 95% conf interval, based on student's t-distribution.\"\"\"\n",
    "        if len(errors) == 1:\n",
    "                return np.inf\n",
    "        else:\n",
    "                return (t.ppf(0.95, len(errors) - 1) * np.std(errors) / np.sqrt(len(errors)))\n",
    "\n",
    "def plus_minus_year(before_year, pre_filtered_data):\n",
    "    \"\"\"For a given year, returns values for expected error for every poll, based on all years before that year\"\"\"\n",
    "    #takes only the years and columns we care about\n",
    "    previous_years = pre_filtered_data.loc[(pre_filtered_data['year'] < before_year) & (pre_filtered_data['year'] >= before_year - years_to_rate), :]\n",
    "    \n",
    "    filtered_data = previous_years.loc[:, ['office_type', 'methodology', 'partisan', 'sample_size', 'days_before_election', 'error']]\n",
    "    \n",
    "    #Splitting up by methodology\n",
    "    for method in methodologies:\n",
    "        filtered_data[method] = filtered_data['methodology'].str.contains(method)\n",
    "    \n",
    "    filtered_data.drop(columns = ['methodology'])\n",
    "    X = filtered_data.drop(columns=['error'])\n",
    "    y = filtered_data['error']\n",
    "    \n",
    "    #Getting error differentials for each pollster\n",
    "    estimator = get_best_estimator(X, y)\n",
    "    previous_years.loc[:, 'expected_error'] = estimator.predict(X)\n",
    "    previous_years.loc[:, 'error_differential'] = previous_years['expected_error'] - previous_years['error']\n",
    "    \n",
    "    pollster_error_differential = previous_years.groupby([\"pollster_rating_id\"], as_index=False).agg({'error_differential': [conf_width, 'mean', 'count'], \n",
    "                                                                                                                  'bias': 'mean'})\n",
    "    \n",
    "    #Calculating info relating to error differentials\n",
    "    pollster_error_differential.columns = [\"pollster_rating_id\", \"error_differential_conf\", \"error_differential_mean\", 'count', 'mean_bias']\n",
    "    #Getting the lower bound for error differential, based on the confidence interval and mean\n",
    "    pollster_error_differential['lower_error_diff'] = pollster_error_differential[\"error_differential_mean\"] - pollster_error_differential[\"error_differential_conf\"]\n",
    "    #Check if a pollster is valid yes or no    \n",
    "    pollster_error_differential['valid'] = pollster_error_differential['count'] >= 10\n",
    "    pollster_error_differential['year'] = before_year\n",
    "    \n",
    "    results = pollster_error_differential.loc[:, ['year', 'pollster_rating_id', \"lower_error_diff\", \"mean_bias\", \"count\", \"valid\"]]\n",
    "    \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Running Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.1174580691027085\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.0205798946596545\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.04354519490235\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.1500743993063955\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 2.932063278732641\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.0282062416076667\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 2.952465621375303\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.0505516587144066\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.0950932929503856\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.099793147868101\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.0071060650018935\n",
      "Fitting 4 folds for each of 75 candidates, totalling 300 fits\n",
      "Test MAE is 3.0937446264443054\n"
     ]
    }
   ],
   "source": [
    "past_polls = pd.read_csv('../../cleaned_data/Historical Polls.csv')\n",
    "full_pollster_ratings = pd.concat([plus_minus_year(year, past_polls) for year in [2002, 2004, 2006, 2008, 2010, 2012, 2014, 2016, 2018, 2020, 2022, 2024]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ranking Pollsters within each year and valid/not, and then publishing full data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pollster_ratings['rank'] = full_pollster_ratings.groupby(['year', 'valid'])['lower_error_diff'].rank('min', ascending=False)\n",
    "full_pollster_ratings.to_csv(\"../../cleaned_data/Pollster Ratings.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
