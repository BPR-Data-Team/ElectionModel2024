{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
    "import re\n",
    "import pickle as pkl\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from hyperopt.pyll.base import Apply\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.plots import plot_convergence\n",
    "\n",
    "#Creating a custom time series cross-validator\n",
    "class CustomTimeSeriesCV(BaseCrossValidator):\n",
    "    \"\"\"Creates an iterator that contains the indices from each dataset based on the years given\"\"\"\n",
    "    def __init__(self, years):\n",
    "        self.years = years\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        for train_years, test_years in self.years:\n",
    "            train_indices = np.where(X['year'].isin(train_years))[0]\n",
    "            test_indices = np.where(X['year'].isin(test_years))[0]\n",
    "            yield train_indices, test_indices\n",
    "        \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return len(self.years) \n",
    "    \n",
    "#Bootstraps X and y\n",
    "def bootstrap(group, n=None):\n",
    "    if n is None:\n",
    "        n = len(group)\n",
    "    return group.sample(n, replace=True)\n",
    "\n",
    "def penalize_wrong(y_true, y_pred, penalty = 4):\n",
    "    return np.mean(np.abs(y_true - y_pred)*(1+penalty*(np.sign(y_true)\n",
    "                                               != np.sign(y_pred))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "#Categorical features that need to be one-hot encoded    \n",
    "one_hot_fts = ['office_type']\n",
    "\n",
    "#Rating is the only ordinal feature\n",
    "ordinal_fts = ['final_rating']\n",
    "\n",
    "#Rating is the only ordinal feature\n",
    "ordinal_fts = ['final_rating']\n",
    "ordinal_fts_ranking = ['Safe R', 'Likely R', 'Leans R', 'Toss-up', 'Leans D', 'Likely D', 'Safe D']\n",
    "\n",
    "#Cont features that should be pass-throughed (and later scaled)\n",
    "cont_fts = [\n",
    "    \"open_seat\", \"incumbent_differential\", \"special\", \"absenteeexcusereq\", \"pollhours\", \"avgpollhours\", \"minpollhours\",\n",
    "    \"regdeadlines\", \"voteridlaws\", \"novoterid\", \"noallmailvote\", \"noearlyvote\", \"nofelonreg\",\n",
    "    \"nofelonsregafterincar\", \"nonstrictid\", \"nonstrictphoto\", \"nopollplacereg\", \"nopr\", \"nosamedayreg\",\n",
    "    \"nostateholiday\", \"pr16\", \"pr17\", \"pr175\", \"pr60\", \"pr90\", \"strictid\", \"strictphoto\", \"covi_num\",\n",
    "    \"prev_dem_gen_tp\", \"prev_gen_margin\", \"weighted_genpoll\", \"weighted_genpoll_lower\",\n",
    "    \"weighted_genpoll_upper\", \"unweighted_genpoll\", \"mean_specials_differential\", \n",
    "    \"house_chamber_margin\", \"senate_chamber_margin\", \"previous_cci\", \"current_cci\", \"change_cci\",\n",
    "    \"previous_gas\", \"current_gas\", \"change_gas\", \"previous_unemployment\", \"current_unemployment\",\n",
    "    \"change_unemployment\",  \"receipts\", \"from_committee_transfers\", \"disbursements\",\n",
    "    \"to_committee_transfers\", \"beginning_cash\", \"ending_cash\", \"candidate_contributions\",\n",
    "    \"individual_contributions\", \"unconvinced_pct\", \"phone_unweighted\", \"online_unweighted\", \"num_polls\",\n",
    "    \"unweighted_estimate\", \"unweighted_ci_lower\", \"unweighted_ci_upper\", \"weighted_estimate\",\n",
    "    \"weighted_ci_lower\", \"weighted_ci_upper\", \"white_pct\", \"black_pct\", \"asian_pct\", \"hispanic_pct\",\n",
    "    \"median_income\", \"impoverished_pct\", \"median_age\", \"renting_pct\", \"inflation\", \"isMidterm\",\n",
    "    \"genballot_predicted_margin\", \"genballot_predicted_lower\", \"genballot_predicted_upper\",\n",
    "    \"poll_fundamental_agree\",  'receipts_DEM', 'receipts_REP', 'disbursements_DEM', 'disbursements_REP', \n",
    "    'average_genballot', 'genballot_individual_predicted_margin', 'genballot_campaign5_predicted_margin', \n",
    "    'genballot_campaign10_predicted_margin', 'genballot_campaign15_predicted_margin', \n",
    "    'average_genballot_predicted_margin', 'expert_rating_democrat', 'finance_fundamental_agree'\n",
    "]\n",
    "\n",
    "#This model is used twice: once without reverse causality (polls/expert ratings) for campaign finance, one with (for the real preds)\n",
    "cont_fts_no_reverse_causality = [\n",
    "    \"open_seat\", \"incumbent_differential\", \"special\", \"absenteeexcusereq\", \"pollhours\", \"avgpollhours\", \"minpollhours\",\n",
    "    \"regdeadlines\", \"voteridlaws\", \"novoterid\", \"noallmailvote\", \"noearlyvote\", \"nofelonreg\",\n",
    "    \"nofelonsregafterincar\", \"nonstrictid\", \"nonstrictphoto\", \"nopollplacereg\", \"nopr\", \"nosamedayreg\",\n",
    "    \"nostateholiday\", \"pr16\", \"pr17\", \"pr175\", \"pr60\", \"pr90\", \"strictid\", \"strictphoto\", \"covi_num\",\n",
    "    \"prev_dem_gen_tp\", \"prev_gen_margin\", \"weighted_genpoll\", \"weighted_genpoll_lower\",\n",
    "    \"weighted_genpoll_upper\", \"unweighted_genpoll\", \"mean_specials_differential\", \n",
    "    \"house_chamber_margin\", \"senate_chamber_margin\", \"previous_cci\", \"current_cci\", \"change_cci\",\n",
    "    \"previous_gas\", \"current_gas\", \"change_gas\", \"previous_unemployment\", \"current_unemployment\",\n",
    "    \"change_unemployment\",  \"receipts\", \"from_committee_transfers\", \"disbursements\",\n",
    "    \"to_committee_transfers\", \"beginning_cash\", \"ending_cash\", \"candidate_contributions\",\n",
    "    \"individual_contributions\", \"white_pct\", \"black_pct\", \"asian_pct\", \"hispanic_pct\",\n",
    "    \"median_income\", \"impoverished_pct\", \"median_age\", \"renting_pct\", \"inflation\", \"isMidterm\",\n",
    "    \"genballot_predicted_margin\", \"genballot_predicted_lower\", \"genballot_predicted_upper\",\n",
    "     'receipts_DEM', 'receipts_REP', 'disbursements_DEM', 'disbursements_REP', \n",
    "    'average_genballot', 'genballot_individual_predicted_margin', 'genballot_campaign5_predicted_margin', \n",
    "    'genballot_campaign10_predicted_margin', 'genballot_campaign15_predicted_margin', \n",
    "    'average_genballot_predicted_margin', 'expert_rating_democrat', 'finance_fundamental_agree'\n",
    "]\n",
    "\n",
    "def optima_model(model, param_dict, data, **kwargs):\n",
    "    \"\"\"Performs hyperparameter optimization for a a given model, keeping track of loss. \n",
    "    ## Parameters:\n",
    "    model: sklearnable model, like XGBoost or Linreg\n",
    "    param_dict: dictionary of hyperparameters to optimize\n",
    "    X: DataFrame with features\n",
    "    y: Series with target variable\"\"\"\n",
    "\n",
    "    train, _ = data.loc[data['year'] < 2022], data.loc[data['year'] == 2022]\n",
    "\n",
    "    # Create fold structure so we can make a custom cross-validation for time-series\n",
    "    folds = [\n",
    "        (range(2002, 2010, 2), [2010, 2012]),\n",
    "        (range(2002, 2014, 2), [2014, 2016]),\n",
    "        (range(2002, 2018, 2), [2018, 2020])\n",
    "    ]\n",
    "\n",
    "    cv = CustomTimeSeriesCV(folds)\n",
    "        \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts),\n",
    "        ('ord', OrdinalEncoder(categories = [ordinal_fts_ranking], handle_unknown='use_encoded_value', \n",
    "                               unknown_value=np.nan), ordinal_fts),\n",
    "        ('num', 'passthrough', cont_fts)])\n",
    "    \n",
    "    \n",
    "    def objective(params):\n",
    "        \"Function that takes in hyperparameters and returns loss, that Hyperopt will minimize.\"        \n",
    "        testing_loss = []\n",
    "        accuracies = []\n",
    "        for train_idx, test_idx in cv.split(train):\n",
    "            bootstrapped_train = train.iloc[train_idx].groupby(['year', 'office_type']).apply(bootstrap)\n",
    "            X_train = bootstrapped_train.drop(columns = ['margin'])\n",
    "            y_train = bootstrapped_train['margin']\n",
    "            X_test = train.iloc[test_idx].drop(columns = ['margin'])\n",
    "            y_test = train.iloc[test_idx]['margin']\n",
    "                   \n",
    "            reg = model(**params)\n",
    "            pipe = Pipeline(steps = [\n",
    "                ('preprocessing', preprocessor), \n",
    "                ('model', reg)])\n",
    "                                    \n",
    "            \"\"\"Goes through each fold and calculates loss.\"\"\"\n",
    "            pipe.fit(X_train, y_train)\n",
    "            \n",
    "            predictions = pipe.predict(X_test)\n",
    "            testing_loss.append(penalize_wrong(y_test, predictions))\n",
    "            accuracies.append(accuracy_score(np.sign(y_test), np.sign(predictions)))\n",
    "            \n",
    "        return {'loss': np.mean(testing_loss), 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "    \"Hyperopt uses the TPE algorithm to optimize hyperparameters. We use the no_progress_loss function to stop early if we don't see progress.\"\n",
    "    best_params = fmin(fn=objective,\n",
    "                    space=param_dict,\n",
    "                    algo=tpe.suggest,\n",
    "                    trials=Trials(),\n",
    "                    early_stop_fn=no_progress_loss(40))\n",
    "                    \n",
    "    model = model(**best_params, **kwargs)\n",
    "    pipe = Pipeline(steps = [\n",
    "        ('preprocessing', preprocessor), \n",
    "        ('model', model)])\n",
    "    \n",
    "    #Training final model on data prior to and including 2022, so we get the full extent of the data!\n",
    "    X, y = data.loc[data['year'] <= 2022, :].drop(columns = ['margin']), data.loc[data['year'] <= 2022, :]['margin']\n",
    "    \n",
    "    pipe.fit(X, y)\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 114/9223372036854775807 [09:08<12338235460665080:02:08,  4.82s/trial, best loss: 8.30907904814069]\n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.49682499380612044, 'importance_type': 'split', 'learning_rate': 0.10265949335310312, 'max_depth': 7, 'min_child_samples': 92, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 92, 'n_jobs': 8, 'num_leaves': 67, 'objective': None, 'random_state': None, 'reg_alpha': 1.1613433792081649, 'reg_lambda': 0.2031575774534017, 'subsample': 0.7899810635093059, 'subsample_for_bin': 21680, 'subsample_freq': 0, 'drop_rate': 0.4890747447594856, 'min_data_in_bin': 6, 'min_data_in_leaf': 6, 'skip_drop': 0.6158585565963699, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n",
      "  0%|          | 44/9223372036854775807 [03:33<12443079841293471:17:20,  4.86s/trial, best loss: 8.27090097421438] \n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.5662178074142034, 'importance_type': 'split', 'learning_rate': 0.03754621095508099, 'max_depth': 7, 'min_child_samples': 128, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 181, 'n_jobs': 8, 'num_leaves': 61, 'objective': None, 'random_state': None, 'reg_alpha': 1.1325067285208088, 'reg_lambda': 1.0288219344874923, 'subsample': 0.7980913662283747, 'subsample_for_bin': 179946, 'subsample_freq': 0, 'drop_rate': 0.36478943684129567, 'min_data_in_bin': 6, 'min_data_in_leaf': 9, 'skip_drop': 0.8659432475363964, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n",
      "  0%|          | 71/9223372036854775807 [05:16<11414700927242353:46:40,  4.46s/trial, best loss: 8.246234213167662]\n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.45885345306372316, 'importance_type': 'split', 'learning_rate': 0.03459051288166345, 'max_depth': 8, 'min_child_samples': 85, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 175, 'n_jobs': 8, 'num_leaves': 39, 'objective': None, 'random_state': None, 'reg_alpha': 1.312429173388637, 'reg_lambda': 1.4971581331369506, 'subsample': 0.5317613156829429, 'subsample_for_bin': 95958, 'subsample_freq': 0, 'drop_rate': 0.09502257695376716, 'min_data_in_bin': 3, 'min_data_in_leaf': 8, 'skip_drop': 0.8216710102612655, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n",
      "  0%|          | 106/9223372036854775807 [08:22<12153581321759293:26:24,  4.74s/trial, best loss: 8.098790487046777]\n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.7582511630695491, 'importance_type': 'split', 'learning_rate': 0.06375009095616709, 'max_depth': 6, 'min_child_samples': 60, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 197, 'n_jobs': 8, 'num_leaves': 36, 'objective': None, 'random_state': None, 'reg_alpha': 0.3389939546937214, 'reg_lambda': 0.676510487286678, 'subsample': 0.7946496885224603, 'subsample_for_bin': 109427, 'subsample_freq': 0, 'drop_rate': 0.42868485100242953, 'min_data_in_bin': 5, 'min_data_in_leaf': 9, 'skip_drop': 0.6355888940157814, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n",
      "  0%|          | 43/9223372036854775807 [03:21<12019190128642757:58:24,  4.69s/trial, best loss: 8.255075908117568]\n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.5464006868162798, 'importance_type': 'split', 'learning_rate': 0.12529213086331317, 'max_depth': 9, 'min_child_samples': 98, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 168, 'n_jobs': 8, 'num_leaves': 58, 'objective': None, 'random_state': None, 'reg_alpha': 0.017354299901133574, 'reg_lambda': 0.6622575838830371, 'subsample': 0.5881797135162409, 'subsample_for_bin': 61544, 'subsample_freq': 0, 'drop_rate': 0.07196764490636944, 'min_data_in_bin': 4, 'min_data_in_leaf': 3, 'skip_drop': 0.36799623658107594, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n",
      "  0%|          | 47/9223372036854775807 [02:55<9585052126529049:01:52,  3.74s/trial, best loss: 8.364302572910258] \n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.6278799441749818, 'importance_type': 'split', 'learning_rate': 0.12311506648805001, 'max_depth': 5, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 116, 'n_jobs': 8, 'num_leaves': 29, 'objective': None, 'random_state': None, 'reg_alpha': 0.8897249317601625, 'reg_lambda': 0.5269510545438829, 'subsample': 0.6335212404879947, 'subsample_for_bin': 68472, 'subsample_freq': 0, 'drop_rate': 0.15787901996779194, 'min_data_in_bin': 9, 'min_data_in_leaf': 8, 'skip_drop': 0.41856111571951715, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n",
      "  0%|          | 61/9223372036854775807 [04:37<11643995378072207:21:36,  4.54s/trial, best loss: 8.276245639792073] \n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.7794686440946852, 'importance_type': 'split', 'learning_rate': 0.09329161658650899, 'max_depth': 9, 'min_child_samples': 61, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 160, 'n_jobs': 8, 'num_leaves': 26, 'objective': None, 'random_state': None, 'reg_alpha': 0.5786735767207505, 'reg_lambda': 0.3807284179182775, 'subsample': 0.5079244100305971, 'subsample_for_bin': 166397, 'subsample_freq': 0, 'drop_rate': 0.21486376953213215, 'min_data_in_bin': 7, 'min_data_in_leaf': 7, 'skip_drop': 0.6246889260272545, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n",
      "  0%|          | 78/9223372036854775807 [06:21<12536773474853403:18:24,  4.89s/trial, best loss: 8.33245811629645] \n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.5396122213240034, 'importance_type': 'split', 'learning_rate': 0.12130365277101518, 'max_depth': 6, 'min_child_samples': 109, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 183, 'n_jobs': 8, 'num_leaves': 54, 'objective': None, 'random_state': None, 'reg_alpha': 1.2127067799478315, 'reg_lambda': 0.9735143302002717, 'subsample': 0.7471167542715678, 'subsample_for_bin': 22035, 'subsample_freq': 0, 'drop_rate': 0.3500384796144593, 'min_data_in_bin': 4, 'min_data_in_leaf': 6, 'skip_drop': 0.7852854124909289, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n",
      "  0%|          | 73/9223372036854775807 [05:16<11109798528618696:14:56,  4.34s/trial, best loss: 8.23010599723821] \n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.7288240213320826, 'importance_type': 'split', 'learning_rate': 0.12737185941918433, 'max_depth': 9, 'min_child_samples': 21, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 154, 'n_jobs': 8, 'num_leaves': 66, 'objective': None, 'random_state': None, 'reg_alpha': 0.23678940485229197, 'reg_lambda': 0.7052761924076563, 'subsample': 0.7439827274001324, 'subsample_for_bin': 74681, 'subsample_freq': 0, 'drop_rate': 0.4268245133135924, 'min_data_in_bin': 2, 'min_data_in_leaf': 8, 'skip_drop': 0.5248114298307989, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n",
      "  0%|          | 87/9223372036854775807 [06:17<11105887622851203:58:56,  4.33s/trial, best loss: 8.291043787533553]\n",
      "{'boosting_type': 'dart', 'class_weight': None, 'colsample_bytree': 0.627145648878567, 'importance_type': 'split', 'learning_rate': 0.053733318830796094, 'max_depth': 4, 'min_child_samples': 124, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 97, 'n_jobs': 8, 'num_leaves': 48, 'objective': None, 'random_state': None, 'reg_alpha': 0.9046607388503523, 'reg_lambda': 0.7994938734663533, 'subsample': 0.6906040617240896, 'subsample_for_bin': 114052, 'subsample_freq': 0, 'drop_rate': 0.37028206973987965, 'min_data_in_bin': 9, 'min_data_in_leaf': 6, 'skip_drop': 0.8273681239297868, 'monotone_constraints': [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, -1, 1, -1, 0, 1, 1, 1, 1, 1, 1, 0], 'verbosity': -1}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../cleaned_data/Engineered Dataset.csv\")\n",
    "data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), one_hot_fts),\n",
    "        ('ord', OrdinalEncoder(categories = [ordinal_fts_ranking], handle_unknown='use_encoded_value', \n",
    "                               unknown_value=np.nan), ordinal_fts),\n",
    "        ('num', 'passthrough', cont_fts)])\n",
    "\n",
    "names_for_monotonicity = preprocessor.fit(data.drop(columns=['margin'])).get_feature_names_out()\n",
    "\n",
    "positive_monotonic = ['incumbent_differential', 'receipts', 'disbursements', 'disbursements_DEM', 'receipts_DEM', 'individual_contributions_DEM',\n",
    "                                       'genballot_predicted_margin', 'specials_predicted_margin', 'unweighted_estimate', 'unweighted_ci_lower',\n",
    "                                       'unweighted_ci_upper','weighted_estimate', 'weighted_ci_lower', 'weighted_ci_upper',\n",
    "                                       'phone_unweighted', 'online_unweighted', 'genballot_individual_predicted_margin', \n",
    "                                       'genballot_campaign5_predicted_margin', 'genballot_campaign10_predicted_margin', 'genballot_campaign15_predicted_margin',\n",
    "                                       'average_genballot_predicted_margin', 'expert_rating_democrat']\n",
    "\n",
    "negative_monotonic = ['disbursements_REP', 'receipts_REP', 'individual_contributions_REP']\n",
    "\n",
    "positive_monotonic = ['num__' + name for name in positive_monotonic] + ['ord__final_rating']\n",
    "negative_monotonic = ['num__' + name for name in negative_monotonic]\n",
    "\n",
    "monotone_constraints = [1 if name in positive_monotonic else -1 if name in negative_monotonic else 0 for name in names_for_monotonicity]\n",
    "\n",
    "# Define the search space for Hyperopt\n",
    "param_dist_lgbm = {\n",
    "    'boosting_type': 'dart',\n",
    "    'num_leaves': hp.randint('num_leaves', 20, 70),  # Reduced the upper limit, \n",
    "    'n_estimators': hp.randint('n_estimators', 50, 200),  # Increased the range\n",
    "    'learning_rate': hp.loguniform('learning_rate', -5, -2),  # Equivalent to about 0.0001 to 0.01\n",
    "    'subsample_for_bin': hp.randint('subsample_for_bin', 20000, 200000),  # Narrowed the range\n",
    "    'min_data_in_bin': hp.randint('min_data_in_bin', 1, 10), \n",
    "    'min_data_in_leaf': hp.randint('min_data_in_leaf', 1, 10),  # Reduced the upper limit\n",
    "    'min_child_samples': hp.randint('min_child_samples', 20, 150),  # Increased the range for more regularization\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.5),  # Increased upper limit for L1 regularization\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.5),  # Increased upper limit for L2 regularization\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.4, 0.8),  # Reduced the upper limit\n",
    "    'subsample': hp.uniform('subsample', 0.5, 0.8),  # Reduced the upper limit for more randomness\n",
    "    'max_depth': hp.randint('max_depth', 2, 10),  # Added max_depth for additional control\n",
    "    'drop_rate': hp.uniform('drop_rate', 0.05, 0.5),  # Added drop_rate for dart\n",
    "    'skip_drop': hp.uniform('skip_drop', 0.1, 0.9),  # Added skip_drop for dart\n",
    "    \"verbose\": -1,  # Keep verbose to -1 to reduce log clutter,  \n",
    "    'monotone_constraints': monotone_constraints, \n",
    "    'n_jobs': 8\n",
    "}\n",
    "\n",
    "num_trials = 10\n",
    "for idx in range(num_trials):\n",
    "    \n",
    "    trained_lgbm = optima_model(lgb.LGBMRegressor, param_dist_lgbm, data,\n",
    "                                boosting_type = 'dart', monotone_constraints = monotone_constraints, verbosity = -1, \n",
    "                                n_jobs = 8)\n",
    "    print(trained_lgbm.named_steps['model'].get_params())\n",
    "    \n",
    "    file_path = f\"../models/Model_{idx}.pkl\"\n",
    "\n",
    "    # Open a file to write in binary mode????        \n",
    "    with open(file_path, 'wb') as file:\n",
    "        pkl.dump(trained_lgbm, file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
